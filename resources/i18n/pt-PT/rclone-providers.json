{
  "providers": {
    "alias": {
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "remote": {
        "title": "Remote",
        "help": "Remote or path to alias.\n\nCan be \"myremote:path/to/dir\", \"myremote:bucket\", \"myremote:\" or \"/local/path\"."
      }
    },
    "archive": {
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "remote": {
        "title": "Remote",
        "help": "Remote to wrap to read archives from.\n\nNormally should contain a ':' and a path, e.g. \"myremote:path/to/dir\",\n\"myremote:bucket\" or \"myremote:\".\n\nIf this is left empty, then the archive backend will use the root as\nthe remote.\n\nThis means that you can use :archive:remote:path and it will be\nequivalent to setting remote=\"remote:path\".\n"
      }
    },
    "azureblob": {
      "access_tier": {
        "title": "Access Tier",
        "help": "Access tier of blob: hot, cool, cold or archive.\n\nArchived blobs can be restored by setting access tier to hot, cool or\ncold. Leave blank if you intend to use default access tier, which is\nset at account level\n\nIf there is no \"access tier\" specified, rclone doesn't apply any tier.\nrclone performs \"Set Tier\" operation on blobs while uploading, if objects\nare not modified, specifying \"access tier\" to new one will have no effect.\nIf blobs are in \"archive tier\" at remote, trying to perform data transfer\noperations from remote will not be allowed. User should first restore by\ntiering blob to \"Hot\", \"Cool\" or \"Cold\"."
      },
      "account": {
        "title": "Account",
        "help": "Azure Storage Account Name.\n\nSet this to the Azure Storage Account Name in use.\n\nLeave blank to use SAS URL or Emulator, otherwise it needs to be set.\n\nIf this is blank and if env_auth is set it will be read from the\nenvironment variable `AZURE_STORAGE_ACCOUNT_NAME` if possible.\n"
      },
      "archive_tier_delete": {
        "title": "Archive Tier Delete",
        "help": "Delete archive tier blobs before overwriting.\n\nArchive tier blobs cannot be updated. So without this flag, if you\nattempt to update an archive tier blob, then rclone will produce the\nerror:\n\n    can't update archive tier blob without --azureblob-archive-tier-delete\n\nWith this flag set then before rclone attempts to overwrite an archive\ntier blob, it will delete the existing blob before uploading its\nreplacement.  This has the potential for data loss if the upload fails\n(unlike updating a normal blob) and also may cost more since deleting\narchive tier blobs early may be chargable.\n"
      },
      "chunk_size": {
        "title": "Chunk Size",
        "help": "Upload chunk size.\n\nNote that this is stored in memory and there may be up to\n\"--transfers\" * \"--azureblob-upload-concurrency\" chunks stored at once\nin memory."
      },
      "client_certificate_password": {
        "title": "Client Certificate Password",
        "help": "Password for the certificate file (optional).\n\nOptionally set this if using\n- Service principal with certificate\n\nAnd the certificate has a password.\n"
      },
      "client_certificate_path": {
        "title": "Client Certificate Path",
        "help": "Path to a PEM or PKCS12 certificate file including the private key.\n\nSet this if using\n- Service principal with certificate\n"
      },
      "client_id": {
        "title": "Client Id",
        "help": "The ID of the client in use.\n\nSet this if using\n- Service principal with client secret\n- Service principal with certificate\n- User with username and password\n"
      },
      "client_secret": {
        "title": "Client Secret",
        "help": "One of the service principal's client secrets\n\nSet this if using\n- Service principal with client secret\n"
      },
      "client_send_certificate_chain": {
        "title": "Client Send Certificate Chain",
        "help": "Send the certificate chain when using certificate auth.\n\nSpecifies whether an authentication request will include an x5c header\nto support subject name / issuer based authentication. When set to\ntrue, authentication requests include the x5c header.\n\nOptionally set this if using\n- Service principal with certificate\n"
      },
      "copy_concurrency": {
        "title": "Copy Concurrency",
        "help": "Concurrency for multipart copy.\n\nThis is the number of chunks of the same file that are copied\nconcurrently.\n\nThese chunks are not buffered in memory and Microsoft recommends\nsetting this value to greater than 1000 in the azcopy documentation.\n\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-optimize#increase-concurrency\n\nIn tests, copy speed increases almost linearly with copy\nconcurrency."
      },
      "copy_cutoff": {
        "title": "Copy Cutoff",
        "help": "Cutoff for switching to multipart copy.\n\nAny files larger than this that need to be server-side copied will be\ncopied in chunks of chunk_size using the put block list API.\n\nFiles smaller than this limit will be copied with the Copy Blob API."
      },
      "delete_snapshots": {
        "title": "Delete Snapshots",
        "help": "Set to specify how to deal with snapshots on blob deletion."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "directory_markers": {
        "title": "Directory Markers",
        "help": "Upload an empty object with a trailing slash when a new directory is created\n\nEmpty folders are unsupported for bucket based remotes, this option\ncreates an empty object ending with \"/\", to persist the folder.\n\nThis object also has the metadata \"hdi_isfolder = true\" to conform to\nthe Microsoft standard.\n "
      },
      "disable_checksum": {
        "title": "Disable Checksum",
        "help": "Don't store MD5 checksum with object metadata.\n\nNormally rclone will calculate the MD5 checksum of the input before\nuploading it so it can add it to metadata on the object. This is great\nfor data integrity checking but can cause long delays for large files\nto start uploading."
      },
      "disable_instance_discovery": {
        "title": "Disable Instance Discovery",
        "help": "Skip requesting Microsoft Entra instance metadata\n\nThis should be set true only by applications authenticating in\ndisconnected clouds, or private clouds such as Azure Stack.\n\nIt determines whether rclone requests Microsoft Entra instance\nmetadata from `https://login.microsoft.com/` before\nauthenticating.\n\nSetting this to true will skip this request, making you responsible\nfor ensuring the configured authority is valid and trustworthy.\n"
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "endpoint": {
        "title": "Endpoint",
        "help": "Endpoint for the service.\n\nLeave blank normally."
      },
      "env_auth": {
        "title": "Env Auth",
        "help": "Read credentials from runtime (environment variables, CLI or MSI).\n\nSee the [authentication docs](/azureblob#authentication) for full info."
      },
      "key": {
        "title": "Key",
        "help": "Storage Account Shared Key.\n\nLeave blank to use SAS URL or Emulator."
      },
      "list_chunk": {
        "title": "List Chunk",
        "help": "Size of blob list.\n\nThis sets the number of blobs requested in each listing chunk. Default\nis the maximum, 5000. \"List blobs\" requests are permitted 2 minutes\nper megabyte to complete. If an operation is taking longer than 2\nminutes per megabyte on average, it will time out (\n[source](https://docs.microsoft.com/en-us/rest/api/storageservices/setting-timeouts-for-blob-service-operations#exceptions-to-default-timeout-interval)\n). This can be used to limit the number of blobs items to return, to\navoid the time out."
      },
      "memory_pool_flush_time": {
        "title": "Memory Pool Flush Time",
        "help": "How often internal memory buffer pools will be flushed. (no longer used)"
      },
      "memory_pool_use_mmap": {
        "title": "Memory Pool Use Mmap",
        "help": "Whether to use mmap buffers in internal memory pool. (no longer used)"
      },
      "msi_client_id": {
        "title": "Msi Client Id",
        "help": "Object ID of the user-assigned MSI to use, if any.\n\nLeave blank if msi_object_id or msi_mi_res_id specified."
      },
      "msi_mi_res_id": {
        "title": "Msi Mi Res Id",
        "help": "Azure resource ID of the user-assigned MSI to use, if any.\n\nLeave blank if msi_client_id or msi_object_id specified."
      },
      "msi_object_id": {
        "title": "Msi Object Id",
        "help": "Object ID of the user-assigned MSI to use, if any.\n\nLeave blank if msi_client_id or msi_mi_res_id specified."
      },
      "no_check_container": {
        "title": "No Check Container",
        "help": "If set, don't attempt to check the container exists or create it.\n\nThis can be useful when trying to minimise the number of transactions\nrclone does if you know the container exists already.\n"
      },
      "no_head_object": {
        "title": "No Head Object",
        "help": "If set, do not do HEAD before GET when getting objects."
      },
      "password": {
        "title": "Password",
        "help": "The user's password\n\nSet this if using\n- User with username and password\n"
      },
      "public_access": {
        "title": "Public Access",
        "help": "Public access level of a container: blob or container."
      },
      "sas_url": {
        "title": "Sas Url",
        "help": "SAS URL for container level access only.\n\nLeave blank if using account/key or Emulator."
      },
      "service_principal_file": {
        "title": "Service Principal File",
        "help": "Path to file containing credentials for use with a service principal.\n\nLeave blank normally. Needed only if you want to use a service principal instead of interactive login.\n\n    $ az ad sp create-for-rbac --name \"<name>\" \\\n      --role \"Storage Blob Data Owner\" \\\n      --scopes \"/subscriptions/<subscription>/resourceGroups/<resource-group>/providers/Microsoft.Storage/storageAccounts/<storage-account>/blobServices/default/containers/<container>\" \\\n      > azure-principal.json\n\nSee [\"Create an Azure service principal\"](https://docs.microsoft.com/en-us/cli/azure/create-an-azure-service-principal-azure-cli) and [\"Assign an Azure role for access to blob data\"](https://docs.microsoft.com/en-us/azure/storage/common/storage-auth-aad-rbac-cli) pages for more details.\n\nIt may be more convenient to put the credentials directly into the\nrclone config file under the `client_id`, `tenant` and `client_secret`\nkeys instead of setting `service_principal_file`.\n"
      },
      "tenant": {
        "title": "Tenant",
        "help": "ID of the service principal's tenant. Also called its directory ID.\n\nSet this if using\n- Service principal with client secret\n- Service principal with certificate\n- User with username and password\n"
      },
      "upload_concurrency": {
        "title": "Upload Concurrency",
        "help": "Concurrency for multipart uploads.\n\nThis is the number of chunks of the same file that are uploaded\nconcurrently.\n\nIf you are uploading small numbers of large files over high-speed\nlinks and these uploads do not fully utilize your bandwidth, then\nincreasing this may help to speed up the transfers.\n\nIn tests, upload speed increases almost linearly with upload\nconcurrency. For example to fill a gigabit pipe it may be necessary to\nraise this to 64. Note that this will use more memory.\n\nNote that chunks are stored in memory and there may be up to\n\"--transfers\" * \"--azureblob-upload-concurrency\" chunks stored at once\nin memory."
      },
      "upload_cutoff": {
        "title": "Upload Cutoff",
        "help": "Cutoff for switching to chunked upload (<= 256 MiB) (deprecated)."
      },
      "use_az": {
        "title": "Use Az",
        "help": "Use Azure CLI tool az for authentication\n\nSet to use the [Azure CLI tool az](https://learn.microsoft.com/en-us/cli/azure/)\nas the sole means of authentication.\n\nSetting this can be useful if you wish to use the az CLI on a host with\na System Managed Identity that you do not want to use.\n\nDon't set env_auth at the same time.\n"
      },
      "use_copy_blob": {
        "title": "Use Copy Blob",
        "help": "Whether to use the Copy Blob API when copying to the same storage account.\n\nIf true (the default) then rclone will use the Copy Blob API for\ncopies to the same storage account even when the size is above the\ncopy_cutoff.\n\nRclone assumes that the same storage account means the same config\nand does not check for the same storage account in different configs.\n\nThere should be no need to change this value.\n"
      },
      "use_emulator": {
        "title": "Use Emulator",
        "help": "Uses local storage emulator if provided as 'true'.\n\nLeave blank if using real azure storage endpoint."
      },
      "use_msi": {
        "title": "Use Msi",
        "help": "Use a managed service identity to authenticate (only works in Azure).\n\nWhen true, use a [managed service identity](https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/)\nto authenticate to Azure Storage instead of a SAS token or account key.\n\nIf the VM(SS) on which this program is running has a system-assigned identity, it will\nbe used by default. If the resource has no system-assigned but exactly one user-assigned identity,\nthe user-assigned identity will be used by default. If the resource has multiple user-assigned\nidentities, the identity to use must be explicitly specified using exactly one of the msi_object_id,\nmsi_client_id, or msi_mi_res_id parameters."
      },
      "username": {
        "title": "Username",
        "help": "User name (usually an email address)\n\nSet this if using\n- User with username and password\n"
      }
    },
    "azurefiles": {
      "account": {
        "title": "Account",
        "help": "Azure Storage Account Name.\n\nSet this to the Azure Storage Account Name in use.\n\nLeave blank to use SAS URL or connection string, otherwise it needs to be set.\n\nIf this is blank and if env_auth is set it will be read from the\nenvironment variable `AZURE_STORAGE_ACCOUNT_NAME` if possible.\n"
      },
      "chunk_size": {
        "title": "Chunk Size",
        "help": "Upload chunk size.\n\nNote that this is stored in memory and there may be up to\n\"--transfers\" * \"--azurefile-upload-concurrency\" chunks stored at once\nin memory."
      },
      "client_certificate_password": {
        "title": "Client Certificate Password",
        "help": "Password for the certificate file (optional).\n\nOptionally set this if using\n- Service principal with certificate\n\nAnd the certificate has a password.\n"
      },
      "client_certificate_path": {
        "title": "Client Certificate Path",
        "help": "Path to a PEM or PKCS12 certificate file including the private key.\n\nSet this if using\n- Service principal with certificate\n"
      },
      "client_id": {
        "title": "Client Id",
        "help": "The ID of the client in use.\n\nSet this if using\n- Service principal with client secret\n- Service principal with certificate\n- User with username and password\n"
      },
      "client_secret": {
        "title": "Client Secret",
        "help": "One of the service principal's client secrets\n\nSet this if using\n- Service principal with client secret\n"
      },
      "client_send_certificate_chain": {
        "title": "Client Send Certificate Chain",
        "help": "Send the certificate chain when using certificate auth.\n\nSpecifies whether an authentication request will include an x5c header\nto support subject name / issuer based authentication. When set to\ntrue, authentication requests include the x5c header.\n\nOptionally set this if using\n- Service principal with certificate\n"
      },
      "connection_string": {
        "title": "Connection String",
        "help": "Azure Files Connection String."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "disable_instance_discovery": {
        "title": "Disable Instance Discovery",
        "help": "Skip requesting Microsoft Entra instance metadata\nThis should be set true only by applications authenticating in\ndisconnected clouds, or private clouds such as Azure Stack.\nIt determines whether rclone requests Microsoft Entra instance\nmetadata from `https://login.microsoft.com/` before\nauthenticating.\nSetting this to true will skip this request, making you responsible\nfor ensuring the configured authority is valid and trustworthy.\n"
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "endpoint": {
        "title": "Endpoint",
        "help": "Endpoint for the service.\n\nLeave blank normally."
      },
      "env_auth": {
        "title": "Env Auth",
        "help": "Read credentials from runtime (environment variables, CLI or MSI).\n\nSee the [authentication docs](/azurefiles#authentication) for full info."
      },
      "key": {
        "title": "Key",
        "help": "Storage Account Shared Key.\n\nLeave blank to use SAS URL or connection string."
      },
      "max_stream_size": {
        "title": "Max Stream Size",
        "help": "Max size for streamed files.\n\nAzure files needs to know in advance how big the file will be. When\nrclone doesn't know it uses this value instead.\n\nThis will be used when rclone is streaming data, the most common uses are:\n\n- Uploading files with `--vfs-cache-mode off` with `rclone mount`\n- Using `rclone rcat`\n- Copying files with unknown length\n\nYou will need this much free space in the share as the file will be this size temporarily.\n"
      },
      "msi_client_id": {
        "title": "Msi Client Id",
        "help": "Object ID of the user-assigned MSI to use, if any.\n\nLeave blank if msi_object_id or msi_mi_res_id specified."
      },
      "msi_mi_res_id": {
        "title": "Msi Mi Res Id",
        "help": "Azure resource ID of the user-assigned MSI to use, if any.\n\nLeave blank if msi_client_id or msi_object_id specified."
      },
      "msi_object_id": {
        "title": "Msi Object Id",
        "help": "Object ID of the user-assigned MSI to use, if any.\n\nLeave blank if msi_client_id or msi_mi_res_id specified."
      },
      "password": {
        "title": "Password",
        "help": "The user's password\n\nSet this if using\n- User with username and password\n"
      },
      "sas_url": {
        "title": "Sas Url",
        "help": "SAS URL.\n\nLeave blank if using account/key or connection string."
      },
      "service_principal_file": {
        "title": "Service Principal File",
        "help": "Path to file containing credentials for use with a service principal.\n\nLeave blank normally. Needed only if you want to use a service principal instead of interactive login.\n\n    $ az ad sp create-for-rbac --name \"<name>\" \\\n      --role \"Storage Files Data Owner\" \\\n      --scopes \"/subscriptions/<subscription>/resourceGroups/<resource-group>/providers/Microsoft.Storage/storageAccounts/<storage-account>/blobServices/default/containers/<container>\" \\\n      > azure-principal.json\n\nSee [\"Create an Azure service principal\"](https://docs.microsoft.com/en-us/cli/azure/create-an-azure-service-principal-azure-cli) and [\"Assign an Azure role for access to files data\"](https://docs.microsoft.com/en-us/azure/storage/common/storage-auth-aad-rbac-cli) pages for more details.\n\n**NB** this section needs updating for Azure Files - pull requests appreciated!\n\nIt may be more convenient to put the credentials directly into the\nrclone config file under the `client_id`, `tenant` and `client_secret`\nkeys instead of setting `service_principal_file`.\n"
      },
      "share_name": {
        "title": "Share Name",
        "help": "Azure Files Share Name.\n\nThis is required and is the name of the share to access.\n"
      },
      "tenant": {
        "title": "Tenant",
        "help": "ID of the service principal's tenant. Also called its directory ID.\n\nSet this if using\n- Service principal with client secret\n- Service principal with certificate\n- User with username and password\n"
      },
      "upload_concurrency": {
        "title": "Upload Concurrency",
        "help": "Concurrency for multipart uploads.\n\nThis is the number of chunks of the same file that are uploaded\nconcurrently.\n\nIf you are uploading small numbers of large files over high-speed\nlinks and these uploads do not fully utilize your bandwidth, then\nincreasing this may help to speed up the transfers.\n\nNote that chunks are stored in memory and there may be up to\n\"--transfers\" * \"--azurefile-upload-concurrency\" chunks stored at once\nin memory."
      },
      "use_az": {
        "title": "Use Az",
        "help": "Use Azure CLI tool az for authentication\nSet to use the [Azure CLI tool az](https://learn.microsoft.com/en-us/cli/azure/)\nas the sole means of authentication.\nSetting this can be useful if you wish to use the az CLI on a host with\na System Managed Identity that you do not want to use.\nDon't set env_auth at the same time.\n"
      },
      "use_msi": {
        "title": "Use Msi",
        "help": "Use a managed service identity to authenticate (only works in Azure).\n\nWhen true, use a [managed service identity](https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/)\nto authenticate to Azure Storage instead of a SAS token or account key.\n\nIf the VM(SS) on which this program is running has a system-assigned identity, it will\nbe used by default. If the resource has no system-assigned but exactly one user-assigned identity,\nthe user-assigned identity will be used by default. If the resource has multiple user-assigned\nidentities, the identity to use must be explicitly specified using exactly one of the msi_object_id,\nmsi_client_id, or msi_mi_res_id parameters."
      },
      "username": {
        "title": "Username",
        "help": "User name (usually an email address)\n\nSet this if using\n- User with username and password\n"
      }
    },
    "b2": {
      "account": {
        "title": "Account",
        "help": "Account ID or Application Key ID."
      },
      "chunk_size": {
        "title": "Chunk Size",
        "help": "Upload chunk size.\n\nWhen uploading large files, chunk the file into this size.\n\nMust fit in memory. These chunks are buffered in memory and there\nmight a maximum of \"--transfers\" chunks in progress at once.\n\n5,000,000 Bytes is the minimum size."
      },
      "copy_cutoff": {
        "title": "Copy Cutoff",
        "help": "Cutoff for switching to multipart copy.\n\nAny files larger than this that need to be server-side copied will be\ncopied in chunks of this size.\n\nThe minimum is 0 and the maximum is 4.6 GiB."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "disable_checksum": {
        "title": "Disable Checksum",
        "help": "Disable checksums for large (> upload cutoff) files.\n\nNormally rclone will calculate the SHA1 checksum of the input before\nuploading it so it can add it to metadata on the object. This is great\nfor data integrity checking but can cause long delays for large files\nto start uploading."
      },
      "download_auth_duration": {
        "title": "Download Auth Duration",
        "help": "Time before the public link authorization token will expire in s or suffix ms|s|m|h|d.\n\nThis is used in combination with \"rclone link\" for making files\naccessible to the public and sets the duration before the download\nauthorization token will expire.\n\nThe minimum value is 1 second. The maximum value is one week."
      },
      "download_url": {
        "title": "Download Url",
        "help": "Custom endpoint for downloads.\n\nThis is usually set to a Cloudflare CDN URL as Backblaze offers\nfree egress for data downloaded through the Cloudflare network.\nRclone works with private buckets by sending an \"Authorization\" header.\nIf the custom endpoint rewrites the requests for authentication,\ne.g., in Cloudflare Workers, this header needs to be handled properly.\nLeave blank if you want to use the endpoint provided by Backblaze.\n\nThe URL provided here SHOULD have the protocol and SHOULD NOT have\na trailing slash or specify the /file/bucket subpath as rclone will\nrequest files with \"{download_url}/file/{bucket_name}/{path}\".\n\nExample:\n> https://mysubdomain.mydomain.tld\n(No trailing \"/\", \"file\" or \"bucket\")"
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "endpoint": {
        "title": "Endpoint",
        "help": "Endpoint for the service.\n\nLeave blank normally."
      },
      "hard_delete": {
        "title": "Hard Delete",
        "help": "Permanently delete files on remote removal, otherwise hide files."
      },
      "key": {
        "title": "Key",
        "help": "Application Key."
      },
      "lifecycle": {
        "title": "Lifecycle",
        "help": "Set the number of days deleted files should be kept when creating a bucket.\n\nOn bucket creation, this parameter is used to create a lifecycle rule\nfor the entire bucket.\n\nIf lifecycle is 0 (the default) it does not create a lifecycle rule so\nthe default B2 behaviour applies. This is to create versions of files\non delete and overwrite and to keep them indefinitely.\n\nIf lifecycle is >0 then it creates a single rule setting the number of\ndays before a file that is deleted or overwritten is deleted\npermanently. This is known as daysFromHidingToDeleting in the b2 docs.\n\nThe minimum value for this parameter is 1 day.\n\nYou can also enable hard_delete in the config also which will mean\ndeletions won't cause versions but overwrites will still cause\nversions to be made.\n\nSee: [rclone backend lifecycle](#lifecycle) for setting lifecycles after bucket creation.\n"
      },
      "memory_pool_flush_time": {
        "title": "Memory Pool Flush Time",
        "help": "How often internal memory buffer pools will be flushed. (no longer used)"
      },
      "memory_pool_use_mmap": {
        "title": "Memory Pool Use Mmap",
        "help": "Whether to use mmap buffers in internal memory pool. (no longer used)"
      },
      "sse_customer_algorithm": {
        "title": "Sse Customer Algorithm",
        "help": "If using SSE-C, the server-side encryption algorithm used when storing this object in B2."
      },
      "sse_customer_key": {
        "title": "Sse Customer Key",
        "help": "To use SSE-C, you may provide the secret encryption key encoded in a UTF-8 compatible string to encrypt/decrypt your data\n\nAlternatively you can provide --sse-customer-key-base64."
      },
      "sse_customer_key_base64": {
        "title": "Sse Customer Key Base64",
        "help": "To use SSE-C, you may provide the secret encryption key encoded in Base64 format to encrypt/decrypt your data\n\nAlternatively you can provide --sse-customer-key."
      },
      "sse_customer_key_md5": {
        "title": "Sse Customer Key Md5",
        "help": "If using SSE-C you may provide the secret encryption key MD5 checksum (optional).\n\nIf you leave it blank, this is calculated automatically from the sse_customer_key provided.\n"
      },
      "test_mode": {
        "title": "Test Mode",
        "help": "A flag string for X-Bz-Test-Mode header for debugging.\n\nThis is for debugging purposes only. Setting it to one of the strings\nbelow will cause b2 to return specific errors:\n\n  * \"fail_some_uploads\"\n  * \"expire_some_account_authorization_tokens\"\n  * \"force_cap_exceeded\"\n\nThese will be set in the \"X-Bz-Test-Mode\" header which is documented\nin the [b2 integrations checklist](https://www.backblaze.com/docs/cloud-storage-integration-checklist)."
      },
      "upload_concurrency": {
        "title": "Upload Concurrency",
        "help": "Concurrency for multipart uploads.\n\nThis is the number of chunks of the same file that are uploaded\nconcurrently.\n\nNote that chunks are stored in memory and there may be up to\n\"--transfers\" * \"--b2-upload-concurrency\" chunks stored at once\nin memory."
      },
      "upload_cutoff": {
        "title": "Upload Cutoff",
        "help": "Cutoff for switching to chunked upload.\n\nFiles above this size will be uploaded in chunks of \"--b2-chunk-size\".\n\nThis value should be set no larger than 4.657 GiB (== 5 GB)."
      },
      "version_at": {
        "title": "Version At",
        "help": "Show file versions as they were at the specified time.\n\nNote that when using this no file write operations are permitted,\nso you can't upload files or delete them."
      },
      "versions": {
        "title": "Versions",
        "help": "Include old versions in directory listings.\n\nNote that when using this no file write operations are permitted,\nso you can't upload files or delete them."
      }
    },
    "box": {
      "access_token": {
        "title": "Access Token",
        "help": "Box App Primary Access Token\n\nLeave blank normally."
      },
      "auth_url": {
        "title": "Auth Url",
        "help": "Auth server URL.\n\nLeave blank to use the provider defaults."
      },
      "box_config_file": {
        "title": "Box Config File",
        "help": "Box App config.json location\n\nLeave blank normally.\n\nLeading `~` will be expanded in the file name as will environment variables such as `${RCLONE_CONFIG_DIR}`."
      },
      "box_sub_type": {
        "title": "Box Sub Type",
        "help": ""
      },
      "client_credentials": {
        "title": "Client Credentials",
        "help": "Use client credentials OAuth flow.\n\nThis will use the OAUTH2 client Credentials Flow as described in RFC 6749.\n\nNote that this option is NOT supported by all backends."
      },
      "client_id": {
        "title": "Client Id",
        "help": "OAuth Client Id.\n\nLeave blank normally."
      },
      "client_secret": {
        "title": "Client Secret",
        "help": "OAuth Client Secret.\n\nLeave blank normally."
      },
      "commit_retries": {
        "title": "Commit Retries",
        "help": "Max number of times to try committing a multipart file."
      },
      "config_credentials": {
        "title": "Config Credentials",
        "help": "Box App config.json contents.\n\nLeave blank normally."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "impersonate": {
        "title": "Impersonate",
        "help": "Impersonate this user ID when using a service account.\n\nSetting this flag allows rclone, when using a JWT service account, to\nact on behalf of another user by setting the as-user header.\n\nThe user ID is the Box identifier for a user. User IDs can found for\nany user via the GET /users endpoint, which is only available to\nadmins, or by calling the GET /users/me endpoint with an authenticated\nuser session.\n\nSee: https://developer.box.com/guides/authentication/jwt/as-user/\n"
      },
      "list_chunk": {
        "title": "List Chunk",
        "help": "Size of listing chunk 1-1000."
      },
      "owned_by": {
        "title": "Owned By",
        "help": "Only show items owned by the login (email address) passed in."
      },
      "root_folder_id": {
        "title": "Root Folder Id",
        "help": "Fill in for rclone to use a non root folder as its starting point."
      },
      "token": {
        "title": "Token",
        "help": "OAuth Access Token as a JSON blob."
      },
      "token_url": {
        "title": "Token Url",
        "help": "Token server url.\n\nLeave blank to use the provider defaults."
      },
      "upload_cutoff": {
        "title": "Upload Cutoff",
        "help": "Cutoff for switching to multipart upload (>= 50 MiB)."
      }
    },
    "cache": {
      "chunk_clean_interval": {
        "title": "Chunk Clean Interval",
        "help": "How often should the cache perform cleanups of the chunk storage.\n\nThe default value should be ok for most people. If you find that the\ncache goes over \"cache-chunk-total-size\" too often then try to lower\nthis value to force it to perform cleanups more often."
      },
      "chunk_no_memory": {
        "title": "Chunk No Memory",
        "help": "Disable the in-memory cache for storing chunks during streaming.\n\nBy default, cache will keep file data during streaming in RAM as well\nto provide it to readers as fast as possible.\n\nThis transient data is evicted as soon as it is read and the number of\nchunks stored doesn't exceed the number of workers. However, depending\non other settings like \"cache-chunk-size\" and \"cache-workers\" this footprint\ncan increase if there are parallel streams too (multiple files being read\nat the same time).\n\nIf the hardware permits it, use this feature to provide an overall better\nperformance during streaming but it can also be disabled if RAM is not\navailable on the local machine."
      },
      "chunk_path": {
        "title": "Chunk Path",
        "help": "Directory to cache chunk files.\n\nPath to where partial file data (chunks) are stored locally. The remote\nname is appended to the final path.\n\nThis config follows the \"--cache-db-path\". If you specify a custom\nlocation for \"--cache-db-path\" and don't specify one for \"--cache-chunk-path\"\nthen \"--cache-chunk-path\" will use the same path as \"--cache-db-path\"."
      },
      "chunk_size": {
        "title": "Chunk Size",
        "help": "The size of a chunk (partial file data).\n\nUse lower numbers for slower connections. If the chunk size is\nchanged, any downloaded chunks will be invalid and cache-chunk-path\nwill need to be cleared or unexpected EOF errors will occur."
      },
      "chunk_total_size": {
        "title": "Chunk Total Size",
        "help": "The total size that the chunks can take up on the local disk.\n\nIf the cache exceeds this value then it will start to delete the\noldest chunks until it goes under this value."
      },
      "db_path": {
        "title": "Db Path",
        "help": "Directory to store file structure metadata DB.\n\nThe remote name is used as the DB file name."
      },
      "db_purge": {
        "title": "Db Purge",
        "help": "Clear all the cached data for this remote on start."
      },
      "db_wait_time": {
        "title": "Db Wait Time",
        "help": "How long to wait for the DB to be available - 0 is unlimited.\n\nOnly one process can have the DB open at any one time, so rclone waits\nfor this duration for the DB to become available before it gives an\nerror.\n\nIf you set it to 0 then it will wait forever."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "info_age": {
        "title": "Info Age",
        "help": "How long to cache file structure information (directory listings, file size, times, etc.). \nIf all write operations are done through the cache then you can safely make\nthis value very large as the cache store will also be updated in real time."
      },
      "plex_insecure": {
        "title": "Plex Insecure",
        "help": "Skip all certificate verification when connecting to the Plex server."
      },
      "plex_password": {
        "title": "Plex Password",
        "help": "The password of the Plex user."
      },
      "plex_token": {
        "title": "Plex Token",
        "help": "The plex token for authentication - auto set normally."
      },
      "plex_url": {
        "title": "Plex Url",
        "help": "The URL of the Plex server."
      },
      "plex_username": {
        "title": "Plex Username",
        "help": "The username of the Plex user."
      },
      "read_retries": {
        "title": "Read Retries",
        "help": "How many times to retry a read from a cache storage.\n\nSince reading from a cache stream is independent from downloading file\ndata, readers can get to a point where there's no more data in the\ncache.  Most of the times this can indicate a connectivity issue if\ncache isn't able to provide file data anymore.\n\nFor really slow connections, increase this to a point where the stream is\nable to provide data but your experience will be very stuttering."
      },
      "remote": {
        "title": "Remote",
        "help": "Remote to cache.\n\nNormally should contain a ':' and a path, e.g. \"myremote:path/to/dir\",\n\"myremote:bucket\" or maybe \"myremote:\" (not recommended)."
      },
      "rps": {
        "title": "Rps",
        "help": "Limits the number of requests per second to the source FS (-1 to disable).\n\nThis setting places a hard limit on the number of requests per second\nthat cache will be doing to the cloud provider remote and try to\nrespect that value by setting waits between reads.\n\nIf you find that you're getting banned or limited on the cloud\nprovider through cache and know that a smaller number of requests per\nsecond will allow you to work with it then you can use this setting\nfor that.\n\nA good balance of all the other settings should make this setting\nuseless but it is available to set for more special cases.\n\n**NOTE**: This will limit the number of requests during streams but\nother API calls to the cloud provider like directory listings will\nstill pass."
      },
      "tmp_upload_path": {
        "title": "Tmp Upload Path",
        "help": "Directory to keep temporary files until they are uploaded.\n\nThis is the path where cache will use as a temporary storage for new\nfiles that need to be uploaded to the cloud provider.\n\nSpecifying a value will enable this feature. Without it, it is\ncompletely disabled and files will be uploaded directly to the cloud\nprovider"
      },
      "tmp_wait_time": {
        "title": "Tmp Wait Time",
        "help": "How long should files be stored in local cache before being uploaded.\n\nThis is the duration that a file must wait in the temporary location\n_cache-tmp-upload-path_ before it is selected for upload.\n\nNote that only one file is uploaded at a time and it can take longer\nto start the upload if a queue formed for this purpose."
      },
      "workers": {
        "title": "Workers",
        "help": "How many workers should run in parallel to download chunks.\n\nHigher values will mean more parallel processing (better CPU needed)\nand more concurrent requests on the cloud provider.  This impacts\nseveral aspects like the cloud provider API limits, more stress on the\nhardware that rclone runs on but it also means that streams will be\nmore fluid and data will be available much more faster to readers.\n\n**Note**: If the optional Plex integration is enabled then this\nsetting will adapt to the type of reading performed and the value\nspecified here will be used as a maximum number of workers to use."
      },
      "writes": {
        "title": "Writes",
        "help": "Cache file data on writes through the FS.\n\nIf you need to read files immediately after you upload them through\ncache you can enable this flag to have their data stored in the\ncache store at the same time during upload."
      }
    },
    "chunker": {
      "chunk_size": {
        "title": "Chunk Size",
        "help": "Files larger than chunk size will be split in chunks."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "fail_hard": {
        "title": "Fail Hard",
        "help": "Choose how chunker should handle files with missing or invalid chunks."
      },
      "hash_type": {
        "title": "Hash Type",
        "help": "Choose how chunker handles hash sums.\n\nAll modes but \"none\" require metadata."
      },
      "meta_format": {
        "title": "Meta Format",
        "help": "Format of the metadata object or \"none\".\n\nBy default \"simplejson\".\nMetadata is a small JSON file named after the composite file."
      },
      "name_format": {
        "title": "Name Format",
        "help": "String format of chunk file names.\n\nThe two placeholders are: base file name (*) and chunk number (#...).\nThere must be one and only one asterisk and one or more consecutive hash characters.\nIf chunk number has less digits than the number of hashes, it is left-padded by zeros.\nIf there are more digits in the number, they are left as is.\nPossible chunk files are ignored if their name does not match given format."
      },
      "remote": {
        "title": "Remote",
        "help": "Remote to chunk/unchunk.\n\nNormally should contain a ':' and a path, e.g. \"myremote:path/to/dir\",\n\"myremote:bucket\" or maybe \"myremote:\" (not recommended)."
      },
      "start_from": {
        "title": "Start From",
        "help": "Minimum valid chunk number. Usually 0 or 1.\n\nBy default chunk numbers start from 1."
      },
      "transactions": {
        "title": "Transactions",
        "help": "Choose how chunker should handle temporary files during transactions."
      }
    },
    "cloudinary": {
      "adjust_media_files_extensions": {
        "title": "Adjust Media Files Extensions",
        "help": "Cloudinary handles media formats as a file attribute and strips it from the name, which is unlike most other file systems"
      },
      "api_key": {
        "title": "Api Key",
        "help": "Cloudinary API Key"
      },
      "api_secret": {
        "title": "Api Secret",
        "help": "Cloudinary API Secret"
      },
      "cloud_name": {
        "title": "Cloud Name",
        "help": "Cloudinary Environment Name"
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "eventually_consistent_delay": {
        "title": "Eventually Consistent Delay",
        "help": "Wait N seconds for eventual consistency of the databases that support the backend operation"
      },
      "media_extensions": {
        "title": "Media Extensions",
        "help": "Cloudinary supported media extensions"
      },
      "upload_prefix": {
        "title": "Upload Prefix",
        "help": "Specify the API endpoint for environments out of the US"
      },
      "upload_preset": {
        "title": "Upload Preset",
        "help": "Upload Preset to select asset manipulation on upload"
      }
    },
    "combine": {
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "upstreams": {
        "title": "Upstreams",
        "help": "Upstreams for combining\n\nThese should be in the form\n\n    dir=remote:path dir2=remote2:path\n\nWhere before the = is specified the root directory and after is the remote to\nput there.\n\nEmbedded spaces can be added using quotes\n\n    \"dir=remote:path with space\" \"dir2=remote2:path with space\"\n\n"
      }
    },
    "compress": {
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "level": {
        "title": "Level",
        "help": "GZIP (levels -2 to 9):\n- -2 — Huffman encoding only. Only use if you know what you're doing.\n- -1 (default) — recommended; equivalent to level 5.\n- 0 — turns off compression.\n- 1–9 — increase compression at the cost of speed. Going past 6 generally offers very little return.\n \nZSTD (levels 0 to 4):\n- 0 — turns off compression entirely.\n- 1 — fastest compression with the lowest ratio.\n- 2 (default) — good balance of speed and compression.\n- 3 — better compression, but uses about 2–3x more CPU than the default.\n- 4 — best possible compression ratio (highest CPU cost).\n \nNotes:\n- Choose GZIP for wide compatibility; ZSTD for better speed/ratio tradeoffs.\n- Negative gzip levels: -2 = Huffman-only, -1 = default (≈ level 5)."
      },
      "mode": {
        "title": "Mode",
        "help": "Compression mode."
      },
      "ram_cache_limit": {
        "title": "Ram Cache Limit",
        "help": "Some remotes don't allow the upload of files with unknown size.\nIn this case the compressed file will need to be cached to determine\nit's size.\n\nFiles smaller than this limit will be cached in RAM, files larger than \nthis limit will be cached on disk."
      },
      "remote": {
        "title": "Remote",
        "help": "Remote to compress."
      }
    },
    "crypt": {
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "directory_name_encryption": {
        "title": "Directory Name Encryption",
        "help": "Option to either encrypt directory names or leave them intact.\n\nNB If filename_encryption is \"off\" then this option will do nothing."
      },
      "filename_encoding": {
        "title": "Filename Encoding",
        "help": "How to encode the encrypted filename to text string.\n\nThis option could help with shortening the encrypted filename. The \nsuitable option would depend on the way your remote count the filename\nlength and if it's case sensitive."
      },
      "filename_encryption": {
        "title": "Filename Encryption",
        "help": "How to encrypt the filenames."
      },
      "no_data_encryption": {
        "title": "No Data Encryption",
        "help": "Option to either encrypt file data or leave it unencrypted."
      },
      "pass_bad_blocks": {
        "title": "Pass Bad Blocks",
        "help": "If set this will pass bad blocks through as all 0.\n\nThis should not be set in normal operation, it should only be set if\ntrying to recover an encrypted file with errors and it is desired to\nrecover as much of the file as possible."
      },
      "password": {
        "title": "Password",
        "help": "Password or pass phrase for encryption."
      },
      "password2": {
        "title": "Password2",
        "help": "Password or pass phrase for salt.\n\nOptional but recommended.\nShould be different to the previous password."
      },
      "remote": {
        "title": "Remote",
        "help": "Remote to encrypt/decrypt.\n\nNormally should contain a ':' and a path, e.g. \"myremote:path/to/dir\",\n\"myremote:bucket\" or maybe \"myremote:\" (not recommended)."
      },
      "server_side_across_configs": {
        "title": "Server Side Across Configs",
        "help": "Deprecated: use --server-side-across-configs instead.\n\nAllow server-side operations (e.g. copy) to work across different crypt configs.\n\nNormally this option is not what you want, but if you have two crypts\npointing to the same backend you can use it.\n\nThis can be used, for example, to change file name encryption type\nwithout re-uploading all the data. Just make two crypt backends\npointing to two different directories with the single changed\nparameter and use rclone move to move the files between the crypt\nremotes."
      },
      "show_mapping": {
        "title": "Show Mapping",
        "help": "For all files listed show how the names encrypt.\n\nIf this flag is set then for each file that the remote is asked to\nlist, it will log (at level INFO) a line stating the decrypted file\nname and the encrypted file name.\n\nThis is so you can work out which encrypted names are which decrypted\nnames just in case you need to do something with the encrypted file\nnames, or for debugging purposes."
      },
      "strict_names": {
        "title": "Strict Names",
        "help": "If set, this will raise an error when crypt comes across a filename that can't be decrypted.\n\n(By default, rclone will just log a NOTICE and continue as normal.)\nThis can happen if encrypted and unencrypted files are stored in the same\ndirectory (which is not recommended.) It may also indicate a more serious\nproblem that should be investigated."
      },
      "suffix": {
        "title": "Suffix",
        "help": "If this is set it will override the default suffix of \".bin\".\n\nSetting suffix to \"none\" will result in an empty suffix. This may be useful \nwhen the path length is critical."
      }
    },
    "doi": {
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "doi": {
        "title": "Doi",
        "help": "The DOI or the doi.org URL."
      },
      "doi_resolver_api_url": {
        "title": "Doi Resolver Api Url",
        "help": "The URL of the DOI resolver API to use.\n\nThe DOI resolver can be set for testing or for cases when the the canonical DOI resolver API cannot be used.\n\nDefaults to \"https://doi.org/api\"."
      },
      "provider": {
        "title": "Provider",
        "help": "DOI provider.\n\nThe DOI provider can be set when rclone does not automatically recognize a supported DOI provider."
      }
    },
    "drive": {
      "acknowledge_abuse": {
        "title": "Acknowledge Abuse",
        "help": "Set to allow files which return cannotDownloadAbusiveFile to be downloaded.\n\nIf downloading a file returns the error \"This file has been identified\nas malware or spam and cannot be downloaded\" with the error code\n\"cannotDownloadAbusiveFile\" then supply this flag to rclone to\nindicate you acknowledge the risks of downloading the file and rclone\nwill download it anyway.\n\nNote that if you are using service account it will need Manager\npermission (not Content Manager) to for this flag to work. If the SA\ndoes not have the right permission, Google will just ignore the flag."
      },
      "allow_import_name_change": {
        "title": "Allow Import Name Change",
        "help": "Allow the filetype to change when uploading Google docs.\n\nE.g. file.doc to file.docx. This will confuse sync and reupload every time."
      },
      "alternate_export": {
        "title": "Alternate Export",
        "help": "Deprecated: No longer needed."
      },
      "auth_owner_only": {
        "title": "Auth Owner Only",
        "help": "Only consider files owned by the authenticated user."
      },
      "auth_url": {
        "title": "Auth Url",
        "help": "Auth server URL.\n\nLeave blank to use the provider defaults."
      },
      "chunk_size": {
        "title": "Chunk Size",
        "help": "Upload chunk size.\n\nMust a power of 2 >= 256k.\n\nMaking this larger will improve performance, but note that each chunk\nis buffered in memory one per transfer.\n\nReducing this will reduce memory usage but decrease performance."
      },
      "client_credentials": {
        "title": "Client Credentials",
        "help": "Use client credentials OAuth flow.\n\nThis will use the OAUTH2 client Credentials Flow as described in RFC 6749.\n\nNote that this option is NOT supported by all backends."
      },
      "client_id": {
        "title": "Client Id",
        "help": "Google Application Client Id\nSetting your own is recommended.\nSee https://rclone.org/drive/#making-your-own-client-id for how to create your own.\nIf you leave this blank, it will use an internal key which is low performance."
      },
      "client_secret": {
        "title": "Client Secret",
        "help": "OAuth Client Secret.\n\nLeave blank normally."
      },
      "copy_shortcut_content": {
        "title": "Copy Shortcut Content",
        "help": "Server side copy contents of shortcuts instead of the shortcut.\n\nWhen doing server side copies, normally rclone will copy shortcuts as\nshortcuts.\n\nIf this flag is used then rclone will copy the contents of shortcuts\nrather than shortcuts themselves when doing server side copies."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "disable_http2": {
        "title": "Disable Http2",
        "help": "Disable drive using http2.\n\nThere is currently an unsolved issue with the google drive backend and\nHTTP/2.  HTTP/2 is therefore disabled by default for the drive backend\nbut can be re-enabled here.  When the issue is solved this flag will\nbe removed.\n\nSee: https://github.com/rclone/rclone/issues/3631\n\n"
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "env_auth": {
        "title": "Env Auth",
        "help": "Get IAM credentials from runtime (environment variables or instance meta data if no env vars).\n\nOnly applies if service_account_file and service_account_credentials is blank."
      },
      "export_formats": {
        "title": "Export Formats",
        "help": "Comma separated list of preferred formats for downloading Google docs."
      },
      "fast_list_bug_fix": {
        "title": "Fast List Bug Fix",
        "help": "Work around a bug in Google Drive listing.\n\nNormally rclone will work around a bug in Google Drive when using\n--fast-list (ListR) where the search \"(A in parents) or (B in\nparents)\" returns nothing sometimes. See #3114, #4289 and\nhttps://issuetracker.google.com/issues/149522397\n\nRclone detects this by finding no items in more than one directory\nwhen listing and retries them as lists of individual directories.\n\nThis means that if you have a lot of empty directories rclone will end\nup listing them all individually and this can take many more API\ncalls.\n\nThis flag allows the work-around to be disabled. This is **not**\nrecommended in normal use - only if you have a particular case you are\nhaving trouble with like many empty directories.\n"
      },
      "formats": {
        "title": "Formats",
        "help": "Deprecated: See export_formats."
      },
      "impersonate": {
        "title": "Impersonate",
        "help": "Impersonate this user when using a service account."
      },
      "import_formats": {
        "title": "Import Formats",
        "help": "Comma separated list of preferred formats for uploading Google docs."
      },
      "keep_revision_forever": {
        "title": "Keep Revision Forever",
        "help": "Keep new head revision of each file forever."
      },
      "list_chunk": {
        "title": "List Chunk",
        "help": "Size of listing chunk 100-1000, 0 to disable."
      },
      "metadata_labels": {
        "title": "Metadata Labels",
        "help": "Control whether labels should be read or written in metadata.\n\nReading labels metadata from files takes an extra API transaction and\nwill slow down listings. It isn't always desirable to set the labels\nfrom the metadata.\n\nThe format of labels is documented in the drive API documentation at\nhttps://developers.google.com/drive/api/reference/rest/v3/Label -\nrclone just provides a JSON dump of this format.\n\nWhen setting labels, the label and fields must already exist - rclone\nwill not create them. This means that if you are transferring labels\nfrom two different accounts you will have to create the labels in\nadvance and use the metadata mapper to translate the IDs between the\ntwo accounts.\n"
      },
      "metadata_owner": {
        "title": "Metadata Owner",
        "help": "Control whether owner should be read or written in metadata.\n\nOwner is a standard part of the file metadata so is easy to read. But it\nisn't always desirable to set the owner from the metadata.\n\nNote that you can't set the owner on Shared Drives, and that setting\nownership will generate an email to the new owner (this can't be\ndisabled), and you can't transfer ownership to someone outside your\norganization.\n"
      },
      "metadata_permissions": {
        "title": "Metadata Permissions",
        "help": "Control whether permissions should be read or written in metadata.\n\nReading permissions metadata from files can be done quickly, but it\nisn't always desirable to set the permissions from the metadata.\n\nNote that rclone drops any inherited permissions on Shared Drives and\nany owner permission on My Drives as these are duplicated in the owner\nmetadata.\n"
      },
      "pacer_burst": {
        "title": "Pacer Burst",
        "help": "Number of API calls to allow without sleeping."
      },
      "pacer_min_sleep": {
        "title": "Pacer Min Sleep",
        "help": "Minimum time to sleep between API calls."
      },
      "resource_key": {
        "title": "Resource Key",
        "help": "Resource key for accessing a link-shared file.\n\nIf you need to access files shared with a link like this\n\n    https://drive.google.com/drive/folders/XXX?resourcekey=YYY&usp=sharing\n\nThen you will need to use the first part \"XXX\" as the \"root_folder_id\"\nand the second part \"YYY\" as the \"resource_key\" otherwise you will get\n404 not found errors when trying to access the directory.\n\nSee: https://developers.google.com/drive/api/guides/resource-keys\n\nThis resource key requirement only applies to a subset of old files.\n\nNote also that opening the folder once in the web interface (with the\nuser you've authenticated rclone with) seems to be enough so that the\nresource key is not needed.\n"
      },
      "root_folder_id": {
        "title": "Root Folder Id",
        "help": "ID of the root folder.\nLeave blank normally.\n\nFill in to access \"Computers\" folders (see docs), or for rclone to use\na non root folder as its starting point.\n"
      },
      "scope": {
        "title": "Scope",
        "help": "Comma separated list of scopes that rclone should use when requesting access from drive."
      },
      "server_side_across_configs": {
        "title": "Server Side Across Configs",
        "help": "Deprecated: use --server-side-across-configs instead.\n\nAllow server-side operations (e.g. copy) to work across different drive configs.\n\nThis can be useful if you wish to do a server-side copy between two\ndifferent Google drives.  Note that this isn't enabled by default\nbecause it isn't easy to tell if it will work between any two\nconfigurations."
      },
      "service_account_credentials": {
        "title": "Service Account Credentials",
        "help": "Service Account Credentials JSON blob.\n\nLeave blank normally.\nNeeded only if you want use SA instead of interactive login."
      },
      "service_account_file": {
        "title": "Service Account File",
        "help": "Service Account Credentials JSON file path.\n\nLeave blank normally.\nNeeded only if you want use SA instead of interactive login.\n\nLeading `~` will be expanded in the file name as will environment variables such as `${RCLONE_CONFIG_DIR}`."
      },
      "shared_with_me": {
        "title": "Shared With Me",
        "help": "Only show files that are shared with me.\n\nInstructs rclone to operate on your \"Shared with me\" folder (where\nGoogle Drive lets you access the files and folders others have shared\nwith you).\n\nThis works both with the \"list\" (lsd, lsl, etc.) and the \"copy\"\ncommands (copy, sync, etc.), and with all other commands too."
      },
      "show_all_gdocs": {
        "title": "Show All Gdocs",
        "help": "Show all Google Docs including non-exportable ones in listings.\n\nIf you try a server side copy on a Google Form without this flag, you\nwill get this error:\n\n    No export formats found for \"application/vnd.google-apps.form\"\n\nHowever adding this flag will allow the form to be server side copied.\n\nNote that rclone doesn't add extensions to the Google Docs file names\nin this mode.\n\nDo **not** use this flag when trying to download Google Docs - rclone\nwill fail to download them.\n"
      },
      "size_as_quota": {
        "title": "Size As Quota",
        "help": "Show sizes as storage quota usage, not actual size.\n\nShow the size of a file as the storage quota used. This is the\ncurrent version plus any older versions that have been set to keep\nforever.\n\n**WARNING**: This flag may have some unexpected consequences.\n\nIt is not recommended to set this flag in your config - the\nrecommended usage is using the flag form --drive-size-as-quota when\ndoing rclone ls/lsl/lsf/lsjson/etc only.\n\nIf you do use this flag for syncing (not recommended) then you will\nneed to use --ignore size also."
      },
      "skip_checksum_gphotos": {
        "title": "Skip Checksum Gphotos",
        "help": "Skip checksums on Google photos and videos only.\n\nUse this if you get checksum errors when transferring Google photos or\nvideos.\n\nSetting this flag will cause Google photos and videos to return a\nblank checksums.\n\nGoogle photos are identified by being in the \"photos\" space.\n\nCorrupted checksums are caused by Google modifying the image/video but\nnot updating the checksum."
      },
      "skip_dangling_shortcuts": {
        "title": "Skip Dangling Shortcuts",
        "help": "If set skip dangling shortcut files.\n\nIf this is set then rclone will not show any dangling shortcuts in listings.\n"
      },
      "skip_gdocs": {
        "title": "Skip Gdocs",
        "help": "Skip google documents in all listings.\n\nIf given, gdocs practically become invisible to rclone."
      },
      "skip_shortcuts": {
        "title": "Skip Shortcuts",
        "help": "If set skip shortcut files.\n\nNormally rclone dereferences shortcut files making them appear as if\nthey are the original file (see [the shortcuts section](#shortcuts)).\nIf this flag is set then rclone will ignore shortcut files completely.\n"
      },
      "starred_only": {
        "title": "Starred Only",
        "help": "Only show files that are starred."
      },
      "stop_on_download_limit": {
        "title": "Stop On Download Limit",
        "help": "Make download limit errors be fatal.\n\nAt the time of writing it is only possible to download 10 TiB of data from\nGoogle Drive a day (this is an undocumented limit). When this limit is\nreached Google Drive produces a slightly different error message. When\nthis flag is set it causes these errors to be fatal.  These will stop\nthe in-progress sync.\n\nNote that this detection is relying on error message strings which\nGoogle don't document so it may break in the future.\n"
      },
      "stop_on_upload_limit": {
        "title": "Stop On Upload Limit",
        "help": "Make upload limit errors be fatal.\n\nAt the time of writing it is only possible to upload 750 GiB of data to\nGoogle Drive a day (this is an undocumented limit). When this limit is\nreached Google Drive produces a slightly different error message. When\nthis flag is set it causes these errors to be fatal.  These will stop\nthe in-progress sync.\n\nNote that this detection is relying on error message strings which\nGoogle don't document so it may break in the future.\n\nSee: https://github.com/rclone/rclone/issues/3857\n"
      },
      "team_drive": {
        "title": "Team Drive",
        "help": "ID of the Shared Drive (Team Drive)."
      },
      "token": {
        "title": "Token",
        "help": "OAuth Access Token as a JSON blob."
      },
      "token_url": {
        "title": "Token Url",
        "help": "Token server url.\n\nLeave blank to use the provider defaults."
      },
      "trashed_only": {
        "title": "Trashed Only",
        "help": "Only show files that are in the trash.\n\nThis will show trashed files in their original directory structure."
      },
      "upload_cutoff": {
        "title": "Upload Cutoff",
        "help": "Cutoff for switching to chunked upload."
      },
      "use_created_date": {
        "title": "Use Created Date",
        "help": "Use file created date instead of modified date.\n\nUseful when downloading data and you want the creation date used in\nplace of the last modified date.\n\n**WARNING**: This flag may have some unexpected consequences.\n\nWhen uploading to your drive all files will be overwritten unless they\nhaven't been modified since their creation. And the inverse will occur\nwhile downloading.  This side effect can be avoided by using the\n\"--checksum\" flag.\n\nThis feature was implemented to retain photos capture date as recorded\nby google photos. You will first need to check the \"Create a Google\nPhotos folder\" option in your google drive settings. You can then copy\nor move the photos locally and use the date the image was taken\n(created) set as the modification date."
      },
      "use_shared_date": {
        "title": "Use Shared Date",
        "help": "Use date file was shared instead of modified date.\n\nNote that, as with \"--drive-use-created-date\", this flag may have\nunexpected consequences when uploading/downloading files.\n\nIf both this flag and \"--drive-use-created-date\" are set, the created\ndate is used."
      },
      "use_trash": {
        "title": "Use Trash",
        "help": "Send files to the trash instead of deleting permanently.\n\nDefaults to true, namely sending files to the trash.\nUse `--drive-use-trash=false` to delete files permanently instead."
      },
      "v2_download_min_size": {
        "title": "V2 Download Min Size",
        "help": "If Object's are greater, use drive v2 API to download."
      }
    },
    "dropbox": {
      "auth_url": {
        "title": "Auth Url",
        "help": "Auth server URL.\n\nLeave blank to use the provider defaults."
      },
      "batch_commit_timeout": {
        "title": "Batch Commit Timeout",
        "help": "Max time to wait for a batch to finish committing. (no longer used)"
      },
      "batch_mode": {
        "title": "Batch Mode",
        "help": "Upload file batching sync|async|off.\n\nThis sets the batch mode used by rclone.\n\nFor full info see [the main docs](https://rclone.org/dropbox/#batch-mode)\n\nThis has 3 possible values\n\n- off - no batching\n- sync - batch uploads and check completion (default)\n- async - batch upload and don't check completion\n\nRclone will close any outstanding batches when it exits which may make\na delay on quit.\n"
      },
      "batch_size": {
        "title": "Batch Size",
        "help": "Max number of files in upload batch.\n\nThis sets the batch size of files to upload. It has to be less than 1000.\n\nBy default this is 0 which means rclone will calculate the batch size\ndepending on the setting of batch_mode.\n\n- batch_mode: async - default batch_size is 100\n- batch_mode: sync - default batch_size is the same as --transfers\n- batch_mode: off - not in use\n\nRclone will close any outstanding batches when it exits which may make\na delay on quit.\n\nSetting this is a great idea if you are uploading lots of small files\nas it will make them a lot quicker. You can use --transfers 32 to\nmaximise throughput.\n"
      },
      "batch_timeout": {
        "title": "Batch Timeout",
        "help": "Max time to allow an idle upload batch before uploading.\n\nIf an upload batch is idle for more than this long then it will be\nuploaded.\n\nThe default for this is 0 which means rclone will choose a sensible\ndefault based on the batch_mode in use.\n\n- batch_mode: async - default batch_timeout is 10s\n- batch_mode: sync - default batch_timeout is 500ms\n- batch_mode: off - not in use\n"
      },
      "chunk_size": {
        "title": "Chunk Size",
        "help": "Upload chunk size (< 150Mi).\n\nAny files larger than this will be uploaded in chunks of this size.\n\nNote that chunks are buffered in memory (one at a time) so rclone can\ndeal with retries.  Setting this larger will increase the speed\nslightly (at most 10% for 128 MiB in tests) at the cost of using more\nmemory.  It can be set smaller if you are tight on memory."
      },
      "client_credentials": {
        "title": "Client Credentials",
        "help": "Use client credentials OAuth flow.\n\nThis will use the OAUTH2 client Credentials Flow as described in RFC 6749.\n\nNote that this option is NOT supported by all backends."
      },
      "client_id": {
        "title": "Client Id",
        "help": "OAuth Client Id.\n\nLeave blank normally."
      },
      "client_secret": {
        "title": "Client Secret",
        "help": "OAuth Client Secret.\n\nLeave blank normally."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "export_formats": {
        "title": "Export Formats",
        "help": "Comma separated list of preferred formats for exporting files\n\nCertain Dropbox files can only be accessed by exporting them to another format.\nThese include Dropbox Paper documents.\n\nFor each such file, rclone will choose the first format on this list that Dropbox\nconsiders valid. If none is valid, it will choose Dropbox's default format.\n\nKnown formats include: \"html\", \"md\" (markdown)"
      },
      "impersonate": {
        "title": "Impersonate",
        "help": "Impersonate this user when using a business account.\n\nNote that if you want to use impersonate, you should make sure this\nflag is set when running \"rclone config\" as this will cause rclone to\nrequest the \"members.read\" scope which it won't normally. This is\nneeded to lookup a members email address into the internal ID that\ndropbox uses in the API.\n\nUsing the \"members.read\" scope will require a Dropbox Team Admin\nto approve during the OAuth flow.\n\nYou will have to use your own App (setting your own client_id and\nclient_secret) to use this option as currently rclone's default set of\npermissions doesn't include \"members.read\". This can be added once\nv1.55 or later is in use everywhere.\n"
      },
      "pacer_min_sleep": {
        "title": "Pacer Min Sleep",
        "help": "Minimum time to sleep between API calls."
      },
      "root_namespace": {
        "title": "Root Namespace",
        "help": "Specify a different Dropbox namespace ID to use as the root for all paths."
      },
      "shared_files": {
        "title": "Shared Files",
        "help": "Instructs rclone to work on individual shared files.\n\nIn this mode rclone's features are extremely limited - only list (ls, lsl, etc.) \noperations and read operations (e.g. downloading) are supported in this mode.\nAll other operations will be disabled."
      },
      "shared_folders": {
        "title": "Shared Folders",
        "help": "Instructs rclone to work on shared folders.\n\t\t\t\nWhen this flag is used with no path only the List operation is supported and \nall available shared folders will be listed. If you specify a path the first part \nwill be interpreted as the name of shared folder. Rclone will then try to mount this \nshared to the root namespace. On success shared folder rclone proceeds normally. \nThe shared folder is now pretty much a normal folder and all normal operations \nare supported. \n\nNote that we don't unmount the shared folder afterwards so the \n--dropbox-shared-folders can be omitted after the first use of a particular \nshared folder.\n\nSee also --dropbox-root-namespace for an alternative way to work with shared\nfolders."
      },
      "show_all_exports": {
        "title": "Show All Exports",
        "help": "Show all exportable files in listings.\n\nAdding this flag will allow all exportable files to be server side copied.\nNote that rclone doesn't add extensions to the exportable file names in this mode.\n\nDo **not** use this flag when trying to download exportable files - rclone\nwill fail to download them.\n"
      },
      "skip_exports": {
        "title": "Skip Exports",
        "help": "Skip exportable files in all listings.\n\nIf given, exportable files practically become invisible to rclone."
      },
      "token": {
        "title": "Token",
        "help": "OAuth Access Token as a JSON blob."
      },
      "token_url": {
        "title": "Token Url",
        "help": "Token server url.\n\nLeave blank to use the provider defaults."
      }
    },
    "fichier": {
      "api_key": {
        "title": "Api Key",
        "help": "Your API Key, get it from https://1fichier.com/console/params.pl."
      },
      "cdn": {
        "title": "Cdn",
        "help": "Set if you wish to use CDN download links."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "file_password": {
        "title": "File Password",
        "help": "If you want to download a shared file that is password protected, add this parameter."
      },
      "folder_password": {
        "title": "Folder Password",
        "help": "If you want to list the files in a shared folder that is password protected, add this parameter."
      },
      "shared_folder": {
        "title": "Shared Folder",
        "help": "If you want to download a shared folder, add this parameter."
      }
    },
    "filefabric": {
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "permanent_token": {
        "title": "Permanent Token",
        "help": "Permanent Authentication Token.\n\nA Permanent Authentication Token can be created in the Enterprise File\nFabric, on the users Dashboard under Security, there is an entry\nyou'll see called \"My Authentication Tokens\". Click the Manage button\nto create one.\n\nThese tokens are normally valid for several years.\n\nFor more info see: https://docs.storagemadeeasy.com/organisationcloud/api-tokens\n"
      },
      "root_folder_id": {
        "title": "Root Folder Id",
        "help": "ID of the root folder.\n\nLeave blank normally.\n\nFill in to make rclone start with directory of a given ID.\n"
      },
      "token": {
        "title": "Token",
        "help": "Session Token.\n\nThis is a session token which rclone caches in the config file. It is\nusually valid for 1 hour.\n\nDon't set this value - rclone will set it automatically.\n"
      },
      "token_expiry": {
        "title": "Token Expiry",
        "help": "Token expiry time.\n\nDon't set this value - rclone will set it automatically.\n"
      },
      "url": {
        "title": "Url",
        "help": "URL of the Enterprise File Fabric to connect to."
      },
      "version": {
        "title": "Version",
        "help": "Version read from the file fabric.\n\nDon't set this value - rclone will set it automatically.\n"
      }
    },
    "filelu": {
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "key": {
        "title": "Key",
        "help": "Your FileLu Rclone key from My Account"
      }
    },
    "filescom": {
      "api_key": {
        "title": "Api Key",
        "help": "The API key used to authenticate with Files.com."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "password": {
        "title": "Password",
        "help": "The password used to authenticate with Files.com."
      },
      "site": {
        "title": "Site",
        "help": "Your site subdomain (e.g. mysite) or custom domain (e.g. myfiles.customdomain.com)."
      },
      "username": {
        "title": "Username",
        "help": "The username used to authenticate with Files.com."
      }
    },
    "ftp": {
      "allow_insecure_tls_ciphers": {
        "title": "Allow Insecure Tls Ciphers",
        "help": "Allow insecure TLS ciphers\n\nSetting this flag will allow the usage of the following TLS ciphers in addition to the secure defaults:\n\n- TLS_RSA_WITH_AES_128_GCM_SHA256\n"
      },
      "ask_password": {
        "title": "Ask Password",
        "help": "Allow asking for FTP password when needed.\n\nIf this is set and no password is supplied then rclone will ask for a password\n"
      },
      "close_timeout": {
        "title": "Close Timeout",
        "help": "Maximum time to wait for a response to close."
      },
      "concurrency": {
        "title": "Concurrency",
        "help": "Maximum number of FTP simultaneous connections, 0 for unlimited.\n\nNote that setting this is very likely to cause deadlocks so it should\nbe used with care.\n\nIf you are doing a sync or copy then make sure concurrency is one more\nthan the sum of `--transfers` and `--checkers`.\n\nIf you use `--check-first` then it just needs to be one more than the\nmaximum of `--checkers` and `--transfers`.\n\nSo for `concurrency 3` you'd use `--checkers 2 --transfers 2\n--check-first` or `--checkers 1 --transfers 1`.\n\n"
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "disable_epsv": {
        "title": "Disable Epsv",
        "help": "Disable using EPSV even if server advertises support."
      },
      "disable_mlsd": {
        "title": "Disable Mlsd",
        "help": "Disable using MLSD even if server advertises support."
      },
      "disable_tls13": {
        "title": "Disable Tls13",
        "help": "Disable TLS 1.3 (workaround for FTP servers with buggy TLS)"
      },
      "disable_utf8": {
        "title": "Disable Utf8",
        "help": "Disable using UTF-8 even if server advertises support."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "explicit_tls": {
        "title": "Explicit Tls",
        "help": "Use Explicit FTPS (FTP over TLS).\n\nWhen using explicit FTP over TLS the client explicitly requests\nsecurity from the server in order to upgrade a plain text connection\nto an encrypted one. Cannot be used in combination with implicit FTPS."
      },
      "force_list_hidden": {
        "title": "Force List Hidden",
        "help": "Use LIST -a to force listing of hidden files and folders. This will disable the use of MLSD."
      },
      "host": {
        "title": "Host",
        "help": "FTP host to connect to.\n\nE.g. \"ftp.example.com\"."
      },
      "http_proxy": {
        "title": "Http Proxy",
        "help": "URL for HTTP CONNECT proxy\n\nSet this to a URL for an HTTP proxy which supports the HTTP CONNECT verb.\n"
      },
      "idle_timeout": {
        "title": "Idle Timeout",
        "help": "Max time before closing idle connections.\n\nIf no connections have been returned to the connection pool in the time\ngiven, rclone will empty the connection pool.\n\nSet to 0 to keep connections indefinitely.\n"
      },
      "no_check_certificate": {
        "title": "No Check Certificate",
        "help": "Do not verify the TLS certificate of the server."
      },
      "no_check_upload": {
        "title": "No Check Upload",
        "help": "Don't check the upload is OK\n\nNormally rclone will try to check the upload exists after it has\nuploaded a file to make sure the size and modification time are as\nexpected.\n\nThis flag stops rclone doing these checks. This enables uploading to\nfolders which are write only.\n\nYou will likely need to use the --inplace flag also if uploading to\na write only folder.\n"
      },
      "pass": {
        "title": "Pass",
        "help": "FTP password."
      },
      "port": {
        "title": "Port",
        "help": "FTP port number."
      },
      "shut_timeout": {
        "title": "Shut Timeout",
        "help": "Maximum time to wait for data connection closing status."
      },
      "socks_proxy": {
        "title": "Socks Proxy",
        "help": "Socks 5 proxy host.\n\t\t\nSupports the format user:pass@host:port, user@host:port, host:port.\n\t\t\nExample:\n\t\t\n    myUser:myPass@localhost:9005\n"
      },
      "tls": {
        "title": "Tls",
        "help": "Use Implicit FTPS (FTP over TLS).\n\nWhen using implicit FTP over TLS the client connects using TLS\nright from the start which breaks compatibility with\nnon-TLS-aware servers. This is usually served over port 990 rather\nthan port 21. Cannot be used in combination with explicit FTPS."
      },
      "tls_cache_size": {
        "title": "Tls Cache Size",
        "help": "Size of TLS session cache for all control and data connections.\n\nTLS cache allows to resume TLS sessions and reuse PSK between connections.\nIncrease if default size is not enough resulting in TLS resumption errors.\nEnabled by default. Use 0 to disable."
      },
      "user": {
        "title": "User",
        "help": "FTP username."
      },
      "writing_mdtm": {
        "title": "Writing Mdtm",
        "help": "Use MDTM to set modification time (VsFtpd quirk)"
      }
    },
    "gofile": {
      "access_token": {
        "title": "Access Token",
        "help": "API Access token\n\nYou can get this from the web control panel."
      },
      "account_id": {
        "title": "Account Id",
        "help": "Account ID\n\nLeave this blank normally, rclone will fill it in automatically.\n"
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "list_chunk": {
        "title": "List Chunk",
        "help": "Number of items to list in each call"
      },
      "root_folder_id": {
        "title": "Root Folder Id",
        "help": "ID of the root folder\n\nLeave this blank normally, rclone will fill it in automatically.\n\nIf you want rclone to be restricted to a particular folder you can\nfill it in - see the docs for more info.\n"
      }
    },
    "google cloud storage": {
      "access_token": {
        "title": "Access Token",
        "help": "Short-lived access token.\n\nLeave blank normally.\nNeeded only if you want use short-lived access token instead of interactive login."
      },
      "anonymous": {
        "title": "Anonymous",
        "help": "Access public buckets and objects without credentials.\n\nSet to 'true' if you just want to download files and don't configure credentials."
      },
      "auth_url": {
        "title": "Auth Url",
        "help": "Auth server URL.\n\nLeave blank to use the provider defaults."
      },
      "bucket_acl": {
        "title": "Bucket Acl",
        "help": "Access Control List for new buckets."
      },
      "bucket_policy_only": {
        "title": "Bucket Policy Only",
        "help": "Access checks should use bucket-level IAM policies.\n\nIf you want to upload objects to a bucket with Bucket Policy Only set\nthen you will need to set this.\n\nWhen it is set, rclone:\n\n- ignores ACLs set on buckets\n- ignores ACLs set on objects\n- creates buckets with Bucket Policy Only set\n\nDocs: https://cloud.google.com/storage/docs/bucket-policy-only\n"
      },
      "client_credentials": {
        "title": "Client Credentials",
        "help": "Use client credentials OAuth flow.\n\nThis will use the OAUTH2 client Credentials Flow as described in RFC 6749.\n\nNote that this option is NOT supported by all backends."
      },
      "client_id": {
        "title": "Client Id",
        "help": "OAuth Client Id.\n\nLeave blank normally."
      },
      "client_secret": {
        "title": "Client Secret",
        "help": "OAuth Client Secret.\n\nLeave blank normally."
      },
      "decompress": {
        "title": "Decompress",
        "help": "If set this will decompress gzip encoded objects.\n\nIt is possible to upload objects to GCS with \"Content-Encoding: gzip\"\nset. Normally rclone will download these files as compressed objects.\n\nIf this flag is set then rclone will decompress these files with\n\"Content-Encoding: gzip\" as they are received. This means that rclone\ncan't check the size and hash but the file contents will be decompressed.\n"
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "directory_markers": {
        "title": "Directory Markers",
        "help": "Upload an empty object with a trailing slash when a new directory is created\n\nEmpty folders are unsupported for bucket based remotes, this option creates an empty\nobject ending with \"/\", to persist the folder.\n"
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "endpoint": {
        "title": "Endpoint",
        "help": "Custom endpoint for the storage API. Leave blank to use the provider default.\n\nWhen using a custom endpoint that includes a subpath (e.g. example.org/custom/endpoint),\nthe subpath will be ignored during upload operations due to a limitation in the\nunderlying Google API Go client library.\nDownload and listing operations will work correctly with the full endpoint path.\nIf you require subpath support for uploads, avoid using subpaths in your custom\nendpoint configuration."
      },
      "env_auth": {
        "title": "Env Auth",
        "help": "Get GCP IAM credentials from runtime (environment variables or instance meta data if no env vars).\n\nOnly applies if service_account_file and service_account_credentials is blank."
      },
      "location": {
        "title": "Location",
        "help": "Location for the newly created buckets."
      },
      "no_check_bucket": {
        "title": "No Check Bucket",
        "help": "If set, don't attempt to check the bucket exists or create it.\n\nThis can be useful when trying to minimise the number of transactions\nrclone does if you know the bucket exists already.\n"
      },
      "object_acl": {
        "title": "Object Acl",
        "help": "Access Control List for new objects."
      },
      "project_number": {
        "title": "Project Number",
        "help": "Project number.\n\nOptional - needed only for list/create/delete buckets - see your developer console."
      },
      "service_account_credentials": {
        "title": "Service Account Credentials",
        "help": "Service Account Credentials JSON blob.\n\nLeave blank normally.\nNeeded only if you want use SA instead of interactive login."
      },
      "service_account_file": {
        "title": "Service Account File",
        "help": "Service Account Credentials JSON file path.\n\nLeave blank normally.\nNeeded only if you want use SA instead of interactive login.\n\nLeading `~` will be expanded in the file name as will environment variables such as `${RCLONE_CONFIG_DIR}`."
      },
      "storage_class": {
        "title": "Storage Class",
        "help": "The storage class to use when storing objects in Google Cloud Storage."
      },
      "token": {
        "title": "Token",
        "help": "OAuth Access Token as a JSON blob."
      },
      "token_url": {
        "title": "Token Url",
        "help": "Token server url.\n\nLeave blank to use the provider defaults."
      },
      "user_project": {
        "title": "User Project",
        "help": "User project.\n\nOptional - needed only for requester pays."
      }
    },
    "google photos": {
      "auth_url": {
        "title": "Auth Url",
        "help": "Auth server URL.\n\nLeave blank to use the provider defaults."
      },
      "batch_commit_timeout": {
        "title": "Batch Commit Timeout",
        "help": "Max time to wait for a batch to finish committing. (no longer used)"
      },
      "batch_mode": {
        "title": "Batch Mode",
        "help": "Upload file batching sync|async|off.\n\nThis sets the batch mode used by rclone.\n\nThis has 3 possible values\n\n- off - no batching\n- sync - batch uploads and check completion (default)\n- async - batch upload and don't check completion\n\nRclone will close any outstanding batches when it exits which may make\na delay on quit.\n"
      },
      "batch_size": {
        "title": "Batch Size",
        "help": "Max number of files in upload batch.\n\nThis sets the batch size of files to upload. It has to be less than 50.\n\nBy default this is 0 which means rclone will calculate the batch size\ndepending on the setting of batch_mode.\n\n- batch_mode: async - default batch_size is 50\n- batch_mode: sync - default batch_size is the same as --transfers\n- batch_mode: off - not in use\n\nRclone will close any outstanding batches when it exits which may make\na delay on quit.\n\nSetting this is a great idea if you are uploading lots of small files\nas it will make them a lot quicker. You can use --transfers 32 to\nmaximise throughput.\n"
      },
      "batch_timeout": {
        "title": "Batch Timeout",
        "help": "Max time to allow an idle upload batch before uploading.\n\nIf an upload batch is idle for more than this long then it will be\nuploaded.\n\nThe default for this is 0 which means rclone will choose a sensible\ndefault based on the batch_mode in use.\n\n- batch_mode: async - default batch_timeout is 10s\n- batch_mode: sync - default batch_timeout is 1s\n- batch_mode: off - not in use\n"
      },
      "client_credentials": {
        "title": "Client Credentials",
        "help": "Use client credentials OAuth flow.\n\nThis will use the OAUTH2 client Credentials Flow as described in RFC 6749.\n\nNote that this option is NOT supported by all backends."
      },
      "client_id": {
        "title": "Client Id",
        "help": "OAuth Client Id.\n\nLeave blank normally."
      },
      "client_secret": {
        "title": "Client Secret",
        "help": "OAuth Client Secret.\n\nLeave blank normally."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "include_archived": {
        "title": "Include Archived",
        "help": "Also view and download archived media.\n\nBy default, rclone does not request archived media. Thus, when syncing,\narchived media is not visible in directory listings or transferred.\n\nNote that media in albums is always visible and synced, no matter\ntheir archive status.\n\nWith this flag, archived media are always visible in directory\nlistings and transferred.\n\nWithout this flag, archived media will not be visible in directory\nlistings and won't be transferred."
      },
      "proxy": {
        "title": "Proxy",
        "help": "Use the gphotosdl proxy for downloading the full resolution images\n\nThe Google API will deliver images and video which aren't full\nresolution, and/or have EXIF data missing.\n\nHowever if you use the gphotosdl proxy then you can download original,\nunchanged images.\n\nThis runs a headless browser in the background.\n\nDownload the software from [gphotosdl](https://github.com/rclone/gphotosdl)\n\nFirst run with\n\n    gphotosdl -login\n\nThen once you have logged into google photos close the browser window\nand run\n\n    gphotosdl\n\nThen supply the parameter `--gphotos-proxy \"http://localhost:8282\"` to make\nrclone use the proxy.\n"
      },
      "read_only": {
        "title": "Read Only",
        "help": "Set to make the Google Photos backend read only.\n\nIf you choose read only then rclone will only request read only access\nto your photos, otherwise rclone will request full access."
      },
      "read_size": {
        "title": "Read Size",
        "help": "Set to read the size of media items.\n\nNormally rclone does not read the size of media items since this takes\nanother transaction.  This isn't necessary for syncing.  However\nrclone mount needs to know the size of files in advance of reading\nthem, so setting this flag when using rclone mount is recommended if\nyou want to read the media."
      },
      "start_year": {
        "title": "Start Year",
        "help": "Year limits the photos to be downloaded to those which are uploaded after the given year."
      },
      "token": {
        "title": "Token",
        "help": "OAuth Access Token as a JSON blob."
      },
      "token_url": {
        "title": "Token Url",
        "help": "Token server url.\n\nLeave blank to use the provider defaults."
      }
    },
    "hasher": {
      "auto_size": {
        "title": "Auto Size",
        "help": "Auto-update checksum for files smaller than this size (disabled by default)."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "hashes": {
        "title": "Hashes",
        "help": "Comma separated list of supported checksum types."
      },
      "max_age": {
        "title": "Max Age",
        "help": "Maximum time to keep checksums in cache (0 = no cache, off = cache forever)."
      },
      "remote": {
        "title": "Remote",
        "help": "Remote to cache checksums for (e.g. myRemote:path)."
      }
    },
    "hdfs": {
      "data_transfer_protection": {
        "title": "Data Transfer Protection",
        "help": "Kerberos data transfer protection: authentication|integrity|privacy.\n\nSpecifies whether or not authentication, data signature integrity\nchecks, and wire encryption are required when communicating with\nthe datanodes. Possible values are 'authentication', 'integrity'\nand 'privacy'. Used only with KERBEROS enabled."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "namenode": {
        "title": "Namenode",
        "help": "Hadoop name nodes and ports.\n\nE.g. \"namenode-1:8020,namenode-2:8020,...\" to connect to host namenodes at port 8020."
      },
      "service_principal_name": {
        "title": "Service Principal Name",
        "help": "Kerberos service principal name for the namenode.\n\nEnables KERBEROS authentication. Specifies the Service Principal Name\n(SERVICE/FQDN) for the namenode. E.g. \\\"hdfs/namenode.hadoop.docker\\\"\nfor namenode running as service 'hdfs' with FQDN 'namenode.hadoop.docker'."
      },
      "username": {
        "title": "Username",
        "help": "Hadoop user name."
      }
    },
    "hidrive": {
      "auth_url": {
        "title": "Auth Url",
        "help": "Auth server URL.\n\nLeave blank to use the provider defaults."
      },
      "chunk_size": {
        "title": "Chunk Size",
        "help": "Chunksize for chunked uploads.\n\nAny files larger than the configured cutoff (or files of unknown size) will be uploaded in chunks of this size.\n\nThe upper limit for this is 2147483647 bytes (about 2.000Gi).\nThat is the maximum amount of bytes a single upload-operation will support.\nSetting this above the upper limit or to a negative value will cause uploads to fail.\n\nSetting this to larger values may increase the upload speed at the cost of using more memory.\nIt can be set to smaller values smaller to save on memory."
      },
      "client_credentials": {
        "title": "Client Credentials",
        "help": "Use client credentials OAuth flow.\n\nThis will use the OAUTH2 client Credentials Flow as described in RFC 6749.\n\nNote that this option is NOT supported by all backends."
      },
      "client_id": {
        "title": "Client Id",
        "help": "OAuth Client Id.\n\nLeave blank normally."
      },
      "client_secret": {
        "title": "Client Secret",
        "help": "OAuth Client Secret.\n\nLeave blank normally."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "disable_fetching_member_count": {
        "title": "Disable Fetching Member Count",
        "help": "Do not fetch number of objects in directories unless it is absolutely necessary.\n\nRequests may be faster if the number of objects in subdirectories is not fetched."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "endpoint": {
        "title": "Endpoint",
        "help": "Endpoint for the service.\n\nThis is the URL that API-calls will be made to."
      },
      "root_prefix": {
        "title": "Root Prefix",
        "help": "The root/parent folder for all paths.\n\nFill in to use the specified folder as the parent for all paths given to the remote.\nThis way rclone can use any folder as its starting point."
      },
      "scope_access": {
        "title": "Scope Access",
        "help": "Access permissions that rclone should use when requesting access from HiDrive."
      },
      "scope_role": {
        "title": "Scope Role",
        "help": "User-level that rclone should use when requesting access from HiDrive."
      },
      "token": {
        "title": "Token",
        "help": "OAuth Access Token as a JSON blob."
      },
      "token_url": {
        "title": "Token Url",
        "help": "Token server url.\n\nLeave blank to use the provider defaults."
      },
      "upload_concurrency": {
        "title": "Upload Concurrency",
        "help": "Concurrency for chunked uploads.\n\nThis is the upper limit for how many transfers for the same file are running concurrently.\nSetting this above to a value smaller than 1 will cause uploads to deadlock.\n\nIf you are uploading small numbers of large files over high-speed links\nand these uploads do not fully utilize your bandwidth, then increasing\nthis may help to speed up the transfers."
      },
      "upload_cutoff": {
        "title": "Upload Cutoff",
        "help": "Cutoff/Threshold for chunked uploads.\n\nAny files larger than this will be uploaded in chunks of the configured chunksize.\n\nThe upper limit for this is 2147483647 bytes (about 2.000Gi).\nThat is the maximum amount of bytes a single upload-operation will support.\nSetting this above the upper limit will cause uploads to fail."
      }
    },
    "http": {
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "headers": {
        "title": "Headers",
        "help": "Set HTTP headers for all transactions.\n\nUse this to set additional HTTP headers for all transactions.\n\nThe input format is comma separated list of key,value pairs.  Standard\n[CSV encoding](https://godoc.org/encoding/csv) may be used.\n\nFor example, to set a Cookie use 'Cookie,name=value', or '\"Cookie\",\"name=value\"'.\n\nYou can set multiple headers, e.g. '\"Cookie\",\"name=value\",\"Authorization\",\"xxx\"'."
      },
      "no_escape": {
        "title": "No Escape",
        "help": "Do not escape URL metacharacters in path names."
      },
      "no_head": {
        "title": "No Head",
        "help": "Don't use HEAD requests.\n\nHEAD requests are mainly used to find file sizes in dir listing.\nIf your site is being very slow to load then you can try this option.\nNormally rclone does a HEAD request for each potential file in a\ndirectory listing to:\n\n- find its size\n- check it really exists\n- check to see if it is a directory\n\nIf you set this option, rclone will not do the HEAD request. This will mean\nthat directory listings are much quicker, but rclone won't have the times or\nsizes of any files, and some files that don't exist may be in the listing."
      },
      "no_slash": {
        "title": "No Slash",
        "help": "Set this if the site doesn't end directories with /.\n\nUse this if your target website does not use / on the end of\ndirectories.\n\nA / on the end of a path is how rclone normally tells the difference\nbetween files and directories.  If this flag is set, then rclone will\ntreat all files with Content-Type: text/html as directories and read\nURLs from them rather than downloading them.\n\nNote that this may cause rclone to confuse genuine HTML files with\ndirectories."
      },
      "url": {
        "title": "Url",
        "help": "URL of HTTP host to connect to.\n\nE.g. \"https://example.com\", or \"https://user:pass@example.com\" to use a username and password."
      }
    },
    "iclouddrive": {
      "apple_id": {
        "title": "Apple Id",
        "help": "Apple ID."
      },
      "client_id": {
        "title": "Client Id",
        "help": "Client id"
      },
      "cookies": {
        "title": "Cookies",
        "help": "cookies (internal use only)"
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "password": {
        "title": "Password",
        "help": "Password."
      },
      "trust_token": {
        "title": "Trust Token",
        "help": "Trust token (internal use)"
      }
    },
    "imagekit": {
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "endpoint": {
        "title": "Endpoint",
        "help": "You can find your ImageKit.io URL endpoint in your [dashboard](https://imagekit.io/dashboard/developer/api-keys)"
      },
      "only_signed": {
        "title": "Only Signed",
        "help": "If you have configured `Restrict unsigned image URLs` in your dashboard settings, set this to true."
      },
      "private_key": {
        "title": "Private Key",
        "help": "You can find your ImageKit.io private key in your [dashboard](https://imagekit.io/dashboard/developer/api-keys)"
      },
      "public_key": {
        "title": "Public Key",
        "help": "You can find your ImageKit.io public key in your [dashboard](https://imagekit.io/dashboard/developer/api-keys)"
      },
      "upload_tags": {
        "title": "Upload Tags",
        "help": "Tags to add to the uploaded files, e.g. \"tag1,tag2\"."
      },
      "versions": {
        "title": "Versions",
        "help": "Include old versions in directory listings."
      }
    },
    "internetarchive": {
      "access_key_id": {
        "title": "Access Key Id",
        "help": "IAS3 Access Key.\n\nLeave blank for anonymous access.\nYou can find one here: https://archive.org/account/s3.php"
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "disable_checksum": {
        "title": "Disable Checksum",
        "help": "Don't ask the server to test against MD5 checksum calculated by rclone.\nNormally rclone will calculate the MD5 checksum of the input before\nuploading it so it can ask the server to check the object against checksum.\nThis is great for data integrity checking but can cause long delays for\nlarge files to start uploading."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "endpoint": {
        "title": "Endpoint",
        "help": "IAS3 Endpoint.\n\nLeave blank for default value."
      },
      "front_endpoint": {
        "title": "Front Endpoint",
        "help": "Host of InternetArchive Frontend.\n\nLeave blank for default value."
      },
      "item_derive": {
        "title": "Item Derive",
        "help": "Whether to trigger derive on the IA item or not. If set to false, the item will not be derived by IA upon upload.\nThe derive process produces a number of secondary files from an upload to make an upload more usable on the web.\nSetting this to false is useful for uploading files that are already in a format that IA can display or reduce burden on IA's infrastructure."
      },
      "item_metadata": {
        "title": "Item Metadata",
        "help": "Metadata to be set on the IA item, this is different from file-level metadata that can be set using --metadata-set.\nFormat is key=value and the 'x-archive-meta-' prefix is automatically added."
      },
      "secret_access_key": {
        "title": "Secret Access Key",
        "help": "IAS3 Secret Key (password).\n\nLeave blank for anonymous access."
      },
      "wait_archive": {
        "title": "Wait Archive",
        "help": "Timeout for waiting the server's processing tasks (specifically archive and book_op) to finish.\nOnly enable if you need to be guaranteed to be reflected after write operations.\n0 to disable waiting. No errors to be thrown in case of timeout."
      }
    },
    "jottacloud": {
      "auth_url": {
        "title": "Auth Url",
        "help": "Auth server URL.\n\nLeave blank to use the provider defaults."
      },
      "client_credentials": {
        "title": "Client Credentials",
        "help": "Use client credentials OAuth flow.\n\nThis will use the OAUTH2 client Credentials Flow as described in RFC 6749.\n\nNote that this option is NOT supported by all backends."
      },
      "client_id": {
        "title": "Client Id",
        "help": "OAuth Client Id.\n\nLeave blank normally."
      },
      "client_secret": {
        "title": "Client Secret",
        "help": "OAuth Client Secret.\n\nLeave blank normally."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "hard_delete": {
        "title": "Hard Delete",
        "help": "Delete files permanently rather than putting them into the trash."
      },
      "md5_memory_limit": {
        "title": "Md5 Memory Limit",
        "help": "Files bigger than this will be cached on disk to calculate the MD5 if required."
      },
      "no_versions": {
        "title": "No Versions",
        "help": "Avoid server side versioning by deleting files and recreating files instead of overwriting them."
      },
      "token": {
        "title": "Token",
        "help": "OAuth Access Token as a JSON blob."
      },
      "token_url": {
        "title": "Token Url",
        "help": "Token server url.\n\nLeave blank to use the provider defaults."
      },
      "trashed_only": {
        "title": "Trashed Only",
        "help": "Only show files that are in the trash.\n\nThis will show trashed files in their original directory structure."
      },
      "upload_resume_limit": {
        "title": "Upload Resume Limit",
        "help": "Files bigger than this can be resumed if the upload fail's."
      }
    },
    "koofr": {
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "endpoint": {
        "title": "Endpoint",
        "help": "The Koofr API endpoint to use."
      },
      "mountid": {
        "title": "Mountid",
        "help": "Mount ID of the mount to use.\n\nIf omitted, the primary mount is used."
      },
      "password": {
        "title": "Password",
        "help": "Your password for rclone generate one at https://app.koofr.net/app/admin/preferences/password."
      },
      "provider": {
        "title": "Provider",
        "help": "Choose your storage provider."
      },
      "setmtime": {
        "title": "Setmtime",
        "help": "Does the backend support setting modification time.\n\nSet this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend."
      },
      "user": {
        "title": "User",
        "help": "Your user name."
      }
    },
    "linkbox": {
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "token": {
        "title": "Token",
        "help": "Token from https://www.linkbox.to/admin/account"
      }
    },
    "local": {
      "case_insensitive": {
        "title": "Case Insensitive",
        "help": "Force the filesystem to report itself as case insensitive.\n\nNormally the local backend declares itself as case insensitive on\nWindows/macOS and case sensitive for everything else.  Use this flag\nto override the default choice."
      },
      "case_sensitive": {
        "title": "Case Sensitive",
        "help": "Force the filesystem to report itself as case sensitive.\n\nNormally the local backend declares itself as case insensitive on\nWindows/macOS and case sensitive for everything else.  Use this flag\nto override the default choice."
      },
      "copy_links": {
        "title": "Copy Links",
        "help": "Follow symlinks and copy the pointed to item."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "hashes": {
        "title": "Hashes",
        "help": "Comma separated list of supported checksum types."
      },
      "links": {
        "title": "Links",
        "help": "Translate symlinks to/from regular files with a '.rclonelink' extension for the local backend."
      },
      "no_check_updated": {
        "title": "No Check Updated",
        "help": "Don't check to see if the files change during upload.\n\nNormally rclone checks the size and modification time of files as they\nare being uploaded and aborts with a message which starts \"can't copy -\nsource file is being updated\" if the file changes during upload.\n\nHowever on some file systems this modification time check may fail (e.g.\n[Glusterfs #2206](https://github.com/rclone/rclone/issues/2206)) so this\ncheck can be disabled with this flag.\n\nIf this flag is set, rclone will use its best efforts to transfer a\nfile which is being updated. If the file is only having things\nappended to it (e.g. a log) then rclone will transfer the log file with\nthe size it had the first time rclone saw it.\n\nIf the file is being modified throughout (not just appended to) then\nthe transfer may fail with a hash check failure.\n\nIn detail, once the file has had stat() called on it for the first\ntime we:\n\n- Only transfer the size that stat gave\n- Only checksum the size that stat gave\n- Don't update the stat info for the file\n\n**NB** do not use this flag on a Windows Volume Shadow (VSS). For some\nunknown reason, files in a VSS sometimes show different sizes from the\ndirectory listing (where the initial stat value comes from on Windows)\nand when stat is called on them directly. Other copy tools always use\nthe direct stat value and setting this flag will disable that.\n"
      },
      "no_clone": {
        "title": "No Clone",
        "help": "Disable reflink cloning for server-side copies.\n\nNormally, for local-to-local transfers, rclone will \"clone\" the file when\npossible, and fall back to \"copying\" only when cloning is not supported.\n\nCloning creates a shallow copy (or \"reflink\") which initially shares blocks with\nthe original file. Unlike a \"hardlink\", the two files are independent and\nneither will affect the other if subsequently modified.\n\nCloning is usually preferable to copying, as it is much faster and is\ndeduplicated by default (i.e. having two identical files does not consume more\nstorage than having just one.)  However, for use cases where data redundancy is\npreferable, --local-no-clone can be used to disable cloning and force \"deep\" copies.\n\nCurrently, cloning is only supported when using APFS on macOS (support for other\nplatforms may be added in the future.)"
      },
      "no_preallocate": {
        "title": "No Preallocate",
        "help": "Disable preallocation of disk space for transferred files.\n\nPreallocation of disk space helps prevent filesystem fragmentation.\nHowever, some virtual filesystem layers (such as Google Drive File\nStream) may incorrectly set the actual file size equal to the\npreallocated space, causing checksum and file size checks to fail.\nUse this flag to disable preallocation."
      },
      "no_set_modtime": {
        "title": "No Set Modtime",
        "help": "Disable setting modtime.\n\nNormally rclone updates modification time of files after they are done\nuploading. This can cause permissions issues on Linux platforms when \nthe user rclone is running as does not own the file uploaded, such as\nwhen copying to a CIFS mount owned by another user. If this option is \nenabled, rclone will no longer update the modtime after copying a file."
      },
      "no_sparse": {
        "title": "No Sparse",
        "help": "Disable sparse files for multi-thread downloads.\n\nOn Windows platforms rclone will make sparse files when doing\nmulti-thread downloads. This avoids long pauses on large files where\nthe OS zeros the file. However sparse files may be undesirable as they\ncause disk fragmentation and can be slow to work with."
      },
      "nounc": {
        "title": "Nounc",
        "help": "Disable UNC (long path names) conversion on Windows."
      },
      "one_file_system": {
        "title": "One File System",
        "help": "Don't cross filesystem boundaries (unix/macOS only)."
      },
      "skip_links": {
        "title": "Skip Links",
        "help": "Don't warn about skipped symlinks.\n\nThis flag disables warning messages on skipped symlinks or junction\npoints, as you explicitly acknowledge that they should be skipped."
      },
      "skip_specials": {
        "title": "Skip Specials",
        "help": "Don't warn about skipped pipes, sockets and device objects.\n\nThis flag disables warning messages on skipped pipes, sockets and\ndevice objects, as you explicitly acknowledge that they should be\nskipped."
      },
      "time_type": {
        "title": "Time Type",
        "help": "Set what kind of time is returned.\n\nNormally rclone does all operations on the mtime or Modification time.\n\nIf you set this flag then rclone will return the Modified time as whatever\nyou set here. So if you use \"rclone lsl --local-time-type ctime\" then\nyou will see ctimes in the listing.\n\nIf the OS doesn't support returning the time_type specified then rclone\nwill silently replace it with the modification time which all OSes support.\n\n- mtime is supported by all OSes\n- atime is supported on all OSes except: plan9, js\n- btime is only supported on: Windows, macOS, freebsd, netbsd\n- ctime is supported on all Oses except: Windows, plan9, js\n\nNote that setting the time will still set the modified time so this is\nonly useful for reading.\n"
      },
      "unicode_normalization": {
        "title": "Unicode Normalization",
        "help": "Apply unicode NFC normalization to paths and filenames.\n\nThis flag can be used to normalize file names into unicode NFC form\nthat are read from the local filesystem.\n\nRclone does not normally touch the encoding of file names it reads from\nthe file system.\n\nThis can be useful when using macOS as it normally provides decomposed (NFD)\nunicode which in some language (eg Korean) doesn't display properly on\nsome OSes.\n\nNote that rclone compares filenames with unicode normalization in the sync\nroutine so this flag shouldn't normally be used."
      },
      "zero_size_links": {
        "title": "Zero Size Links",
        "help": "Assume the Stat size of links is zero (and read them instead) (deprecated).\n\nRclone used to use the Stat size of links as the link size, but this fails in quite a few places:\n\n- Windows\n- On some virtual filesystems (such ash LucidLink)\n- Android\n\nSo rclone now always reads the link.\n"
      }
    },
    "mailru": {
      "auth_url": {
        "title": "Auth Url",
        "help": "Auth server URL.\n\nLeave blank to use the provider defaults."
      },
      "check_hash": {
        "title": "Check Hash",
        "help": "What should copy do if file checksum is mismatched or invalid."
      },
      "client_credentials": {
        "title": "Client Credentials",
        "help": "Use client credentials OAuth flow.\n\nThis will use the OAUTH2 client Credentials Flow as described in RFC 6749.\n\nNote that this option is NOT supported by all backends."
      },
      "client_id": {
        "title": "Client Id",
        "help": "OAuth Client Id.\n\nLeave blank normally."
      },
      "client_secret": {
        "title": "Client Secret",
        "help": "OAuth Client Secret.\n\nLeave blank normally."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "pass": {
        "title": "Pass",
        "help": "Password.\n\nThis must be an app password - rclone will not work with your normal\npassword. See the Configuration section in the docs for how to make an\napp password.\n"
      },
      "quirks": {
        "title": "Quirks",
        "help": "Comma separated list of internal maintenance flags.\n\nThis option must not be used by an ordinary user. It is intended only to\nfacilitate remote troubleshooting of backend issues. Strict meaning of\nflags is not documented and not guaranteed to persist between releases.\nQuirks will be removed when the backend grows stable.\nSupported quirks: atomicmkdir binlist unknowndirs"
      },
      "speedup_enable": {
        "title": "Speedup Enable",
        "help": "Skip full upload if there is another file with same data hash.\n\nThis feature is called \"speedup\" or \"put by hash\". It is especially efficient\nin case of generally available files like popular books, video or audio clips,\nbecause files are searched by hash in all accounts of all mailru users.\nIt is meaningless and ineffective if source file is unique or encrypted.\nPlease note that rclone may need local memory and disk space to calculate\ncontent hash in advance and decide whether full upload is required.\nAlso, if rclone does not know file size in advance (e.g. in case of\nstreaming or partial uploads), it will not even try this optimization."
      },
      "speedup_file_patterns": {
        "title": "Speedup File Patterns",
        "help": "Comma separated list of file name patterns eligible for speedup (put by hash).\n\nPatterns are case insensitive and can contain '*' or '?' meta characters."
      },
      "speedup_max_disk": {
        "title": "Speedup Max Disk",
        "help": "This option allows you to disable speedup (put by hash) for large files.\n\nReason is that preliminary hashing can exhaust your RAM or disk space."
      },
      "speedup_max_memory": {
        "title": "Speedup Max Memory",
        "help": "Files larger than the size given below will always be hashed on disk."
      },
      "token": {
        "title": "Token",
        "help": "OAuth Access Token as a JSON blob."
      },
      "token_url": {
        "title": "Token Url",
        "help": "Token server url.\n\nLeave blank to use the provider defaults."
      },
      "user": {
        "title": "User",
        "help": "User name (usually email)."
      },
      "user_agent": {
        "title": "User Agent",
        "help": "HTTP user agent used internally by client.\n\nDefaults to \"rclone/VERSION\" or \"--user-agent\" provided on command line."
      }
    },
    "mega": {
      "2fa": {
        "title": "2Fa",
        "help": "The 2FA code of your MEGA account if the account is set up with one"
      },
      "debug": {
        "title": "Debug",
        "help": "Output more debug from Mega.\n\nIf this flag is set (along with -vv) it will print further debugging\ninformation from the mega backend."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "hard_delete": {
        "title": "Hard Delete",
        "help": "Delete files permanently rather than putting them into the trash.\n\nNormally the mega backend will put all deletions into the trash rather\nthan permanently deleting them.  If you specify this then rclone will\npermanently delete objects instead."
      },
      "master_key": {
        "title": "Master Key",
        "help": "Master key (internal use only)"
      },
      "pass": {
        "title": "Pass",
        "help": "Password."
      },
      "session_id": {
        "title": "Session Id",
        "help": "Session (internal use only)"
      },
      "use_https": {
        "title": "Use Https",
        "help": "Use HTTPS for transfers.\n\nMEGA uses plain text HTTP connections by default.\nSome ISPs throttle HTTP connections, this causes transfers to become very slow.\nEnabling this will force MEGA to use HTTPS for all transfers.\nHTTPS is normally not necessary since all data is already encrypted anyway.\nEnabling it will increase CPU usage and add network overhead."
      },
      "user": {
        "title": "User",
        "help": "User name."
      }
    },
    "memory": {
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      }
    },
    "netstorage": {
      "account": {
        "title": "Account",
        "help": "Set the NetStorage account name"
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "host": {
        "title": "Host",
        "help": "Domain+path of NetStorage host to connect to.\n\nFormat should be `<domain>/<internal folders>`"
      },
      "protocol": {
        "title": "Protocol",
        "help": "Select between HTTP or HTTPS protocol.\n\nMost users should choose HTTPS, which is the default.\nHTTP is provided primarily for debugging purposes."
      },
      "secret": {
        "title": "Secret",
        "help": "Set the NetStorage account secret/G2O key for authentication.\n\nPlease choose the 'y' option to set your own password then enter your secret."
      }
    },
    "onedrive": {
      "access_scopes": {
        "title": "Access Scopes",
        "help": "Set scopes to be requested by rclone.\n\nChoose or manually enter a custom space separated list with all scopes, that rclone should request.\n"
      },
      "auth_url": {
        "title": "Auth Url",
        "help": "Auth server URL.\n\nLeave blank to use the provider defaults."
      },
      "av_override": {
        "title": "Av Override",
        "help": "Allows download of files the server thinks has a virus.\n\nThe onedrive/sharepoint server may check files uploaded with an Anti\nVirus checker. If it detects any potential viruses or malware it will\nblock download of the file.\n\nIn this case you will see a message like this\n\n    server reports this file is infected with a virus - use --onedrive-av-override to download anyway: Infected (name of virus): 403 Forbidden: \n\nIf you are 100% sure you want to download this file anyway then use\nthe --onedrive-av-override flag, or av_override = true in the config\nfile.\n"
      },
      "chunk_size": {
        "title": "Chunk Size",
        "help": "Chunk size to upload files with - must be multiple of 320k (327,680 bytes).\n\nAbove this size files will be chunked - must be multiple of 320k (327,680 bytes) and\nshould not exceed 250M (262,144,000 bytes) else you may encounter \\\"Microsoft.SharePoint.Client.InvalidClientQueryException: The request message is too big.\\\"\nNote that the chunks will be buffered into memory."
      },
      "client_credentials": {
        "title": "Client Credentials",
        "help": "Use client credentials OAuth flow.\n\nThis will use the OAUTH2 client Credentials Flow as described in RFC 6749.\n\nNote that this option is NOT supported by all backends."
      },
      "client_id": {
        "title": "Client Id",
        "help": "OAuth Client Id.\n\nLeave blank normally."
      },
      "client_secret": {
        "title": "Client Secret",
        "help": "OAuth Client Secret.\n\nLeave blank normally."
      },
      "delta": {
        "title": "Delta",
        "help": "If set rclone will use delta listing to implement recursive listings.\n\nIf this flag is set the onedrive backend will advertise `ListR`\nsupport for recursive listings.\n\nSetting this flag speeds up these things greatly:\n\n    rclone lsf -R onedrive:\n    rclone size onedrive:\n    rclone rc vfs/refresh recursive=true\n\n**However** the delta listing API **only** works at the root of the\ndrive. If you use it not at the root then it recurses from the root\nand discards all the data that is not under the directory you asked\nfor. So it will be correct but may not be very efficient.\n\nThis is why this flag is not set as the default.\n\nAs a rule of thumb if nearly all of your data is under rclone's root\ndirectory (the `root/directory` in `onedrive:root/directory`) then\nusing this flag will be be a big performance win. If your data is\nmostly not under the root then using this flag will be a big\nperformance loss.\n\nIt is recommended if you are mounting your onedrive at the root\n(or near the root when using crypt) and using rclone `rc vfs/refresh`.\n"
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "disable_site_permission": {
        "title": "Disable Site Permission",
        "help": "Disable the request for Sites.Read.All permission.\n\nIf set to true, you will no longer be able to search for a SharePoint site when\nconfiguring drive ID, because rclone will not request Sites.Read.All permission.\nSet it to true if your organization didn't assign Sites.Read.All permission to the\napplication, and your organization disallows users to consent app permission\nrequest on their own."
      },
      "drive_id": {
        "title": "Drive Id",
        "help": "The ID of the drive to use."
      },
      "drive_type": {
        "title": "Drive Type",
        "help": "The type of the drive (personal | business | documentLibrary)."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "expose_onenote_files": {
        "title": "Expose Onenote Files",
        "help": "Set to make OneNote files show up in directory listings.\n\nBy default, rclone will hide OneNote files in directory listings because\noperations like \"Open\" and \"Update\" won't work on them.  But this\nbehaviour may also prevent you from deleting them.  If you want to\ndelete OneNote files or otherwise want them to show up in directory\nlisting, set this option."
      },
      "hard_delete": {
        "title": "Hard Delete",
        "help": "Permanently delete files on removal.\n\nNormally files will get sent to the recycle bin on deletion. Setting\nthis flag causes them to be permanently deleted. Use with care.\n\nOneDrive personal accounts do not support the permanentDelete API,\nit only applies to OneDrive for Business and SharePoint document libraries.\n"
      },
      "hash_type": {
        "title": "Hash Type",
        "help": "Specify the hash in use for the backend.\n\nThis specifies the hash type in use. If set to \"auto\" it will use the\ndefault hash which is QuickXorHash.\n\nBefore rclone 1.62 an SHA1 hash was used by default for Onedrive\nPersonal. For 1.62 and later the default is to use a QuickXorHash for\nall onedrive types. If an SHA1 hash is desired then set this option\naccordingly.\n\nFrom July 2023 QuickXorHash will be the only available hash for\nboth OneDrive for Business and OneDrive Personal.\n\nThis can be set to \"none\" to not use any hashes.\n\nIf the hash requested does not exist on the object, it will be\nreturned as an empty string which is treated as a missing hash by\nrclone.\n"
      },
      "link_password": {
        "title": "Link Password",
        "help": "Set the password for links created by the link command.\n\nAt the time of writing this only works with OneDrive personal paid accounts.\n"
      },
      "link_scope": {
        "title": "Link Scope",
        "help": "Set the scope of the links created by the link command."
      },
      "link_type": {
        "title": "Link Type",
        "help": "Set the type of the links created by the link command."
      },
      "list_chunk": {
        "title": "List Chunk",
        "help": "Size of listing chunk."
      },
      "metadata_permissions": {
        "title": "Metadata Permissions",
        "help": "Control whether permissions should be read or written in metadata.\n\nReading permissions metadata from files can be done quickly, but it\nisn't always desirable to set the permissions from the metadata.\n"
      },
      "no_versions": {
        "title": "No Versions",
        "help": "Remove all versions on modifying operations.\n\nOnedrive for business creates versions when rclone uploads new files\noverwriting an existing one and when it sets the modification time.\n\nThese versions take up space out of the quota.\n\nThis flag checks for versions after file upload and setting\nmodification time and removes all but the last version.\n\n**NB** Onedrive personal can't currently delete versions so don't use\nthis flag there.\n"
      },
      "region": {
        "title": "Region",
        "help": "Choose national cloud region for OneDrive."
      },
      "root_folder_id": {
        "title": "Root Folder Id",
        "help": "ID of the root folder.\n\nThis isn't normally needed, but in special circumstances you might\nknow the folder ID that you wish to access but not be able to get\nthere through a path traversal.\n"
      },
      "server_side_across_configs": {
        "title": "Server Side Across Configs",
        "help": "Deprecated: use --server-side-across-configs instead.\n\nAllow server-side operations (e.g. copy) to work across different onedrive configs.\n\nThis will work if you are copying between two OneDrive *Personal* drives AND the files to\ncopy are already shared between them. Additionally, it should also function for a user who\nhas access permissions both between Onedrive for *business* and *SharePoint* under the *same\ntenant*, and between *SharePoint* and another *SharePoint* under the *same tenant*. In other\ncases, rclone will fall back to normal copy (which will be slightly slower)."
      },
      "tenant": {
        "title": "Tenant",
        "help": "ID of the service principal's tenant. Also called its directory ID.\n\nSet this if using\n- Client Credential flow\n"
      },
      "token": {
        "title": "Token",
        "help": "OAuth Access Token as a JSON blob."
      },
      "token_url": {
        "title": "Token Url",
        "help": "Token server url.\n\nLeave blank to use the provider defaults."
      },
      "upload_cutoff": {
        "title": "Upload Cutoff",
        "help": "Cutoff for switching to chunked upload.\n\nAny files larger than this will be uploaded in chunks of chunk_size.\n\nThis is disabled by default as uploading using single part uploads\ncauses rclone to use twice the storage on Onedrive business as when\nrclone sets the modification time after the upload Onedrive creates a\nnew version.\n\nSee: https://github.com/rclone/rclone/issues/1716\n"
      }
    },
    "opendrive": {
      "access": {
        "title": "Access",
        "help": "Files and folders will be uploaded with this access permission (default private)"
      },
      "chunk_size": {
        "title": "Chunk Size",
        "help": "Files will be uploaded in chunks this size.\n\nNote that these chunks are buffered in memory so increasing them will\nincrease memory use."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "password": {
        "title": "Password",
        "help": "Password."
      },
      "username": {
        "title": "Username",
        "help": "Username."
      }
    },
    "oracleobjectstorage": {
      "attempt_resume_upload": {
        "title": "Attempt Resume Upload",
        "help": "If true attempt to resume previously started multipart upload for the object.\nThis will be helpful to speed up multipart transfers by resuming uploads from past session.\n\nWARNING: If chunk size differs in resumed session from past incomplete session, then the resumed multipart upload is \naborted and a new multipart upload is started with the new chunk size.\n\nThe flag leave_parts_on_error must be true to resume and optimize to skip parts that were already uploaded successfully.\n"
      },
      "chunk_size": {
        "title": "Chunk Size",
        "help": "Chunk size to use for uploading.\n\nWhen uploading files larger than upload_cutoff or files with unknown\nsize (e.g. from \"rclone rcat\" or uploaded with \"rclone mount\" they will be uploaded \nas multipart uploads using this chunk size.\n\nNote that \"upload_concurrency\" chunks of this size are buffered\nin memory per transfer.\n\nIf you are transferring large files over high-speed links and you have\nenough memory, then increasing this will speed up the transfers.\n\nRclone will automatically increase the chunk size when uploading a\nlarge file of known size to stay below the 10,000 chunks limit.\n\nFiles of unknown size are uploaded with the configured\nchunk_size. Since the default chunk size is 5 MiB and there can be at\nmost 10,000 chunks, this means that by default the maximum size of\na file you can stream upload is 48 GiB.  If you wish to stream upload\nlarger files then you will need to increase chunk_size.\n\nIncreasing the chunk size decreases the accuracy of the progress\nstatistics displayed with \"-P\" flag.\n"
      },
      "compartment": {
        "title": "Compartment",
        "help": "Specify compartment OCID, if you need to list buckets.\n\nList objects works without compartment OCID."
      },
      "config_file": {
        "title": "Config File",
        "help": "Path to OCI config file"
      },
      "config_profile": {
        "title": "Config Profile",
        "help": "Profile name inside the oci config file"
      },
      "copy_cutoff": {
        "title": "Copy Cutoff",
        "help": "Cutoff for switching to multipart copy.\n\nAny files larger than this that need to be server-side copied will be\ncopied in chunks of this size.\n\nThe minimum is 0 and the maximum is 5 GiB."
      },
      "copy_timeout": {
        "title": "Copy Timeout",
        "help": "Timeout for copy.\n\nCopy is an asynchronous operation, specify timeout to wait for copy to succeed\n"
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "disable_checksum": {
        "title": "Disable Checksum",
        "help": "Don't store MD5 checksum with object metadata.\n\nNormally rclone will calculate the MD5 checksum of the input before\nuploading it so it can add it to metadata on the object. This is great\nfor data integrity checking but can cause long delays for large files\nto start uploading."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "endpoint": {
        "title": "Endpoint",
        "help": "Endpoint for Object storage API.\n\nLeave blank to use the default endpoint for the region."
      },
      "leave_parts_on_error": {
        "title": "Leave Parts On Error",
        "help": "If true avoid calling abort upload on a failure, leaving all successfully uploaded parts for manual recovery.\n\nIt should be set to true for resuming uploads across different sessions.\n\nWARNING: Storing parts of an incomplete multipart upload counts towards space usage on object storage and will add\nadditional costs if not cleaned up.\n"
      },
      "max_upload_parts": {
        "title": "Max Upload Parts",
        "help": "Maximum number of parts in a multipart upload.\n\nThis option defines the maximum number of multipart chunks to use\nwhen doing a multipart upload.\n\nOCI has max parts limit of 10,000 chunks.\n\nRclone will automatically increase the chunk size when uploading a\nlarge file of a known size to stay below this number of chunks limit.\n"
      },
      "namespace": {
        "title": "Namespace",
        "help": "Object storage namespace"
      },
      "no_check_bucket": {
        "title": "No Check Bucket",
        "help": "If set, don't attempt to check the bucket exists or create it.\n\nThis can be useful when trying to minimise the number of transactions\nrclone does if you know the bucket exists already.\n\nIt can also be needed if the user you are using does not have bucket\ncreation permissions.\n"
      },
      "provider": {
        "title": "Provider",
        "help": "Choose your Auth Provider"
      },
      "region": {
        "title": "Region",
        "help": "Object storage Region"
      },
      "sse_customer_algorithm": {
        "title": "Sse Customer Algorithm",
        "help": "If using SSE-C, the optional header that specifies \"AES256\" as the encryption algorithm.\nObject Storage supports \"AES256\" as the encryption algorithm. For more information, see\nUsing Your Own Keys for Server-Side Encryption (https://docs.cloud.oracle.com/Content/Object/Tasks/usingyourencryptionkeys.htm)."
      },
      "sse_customer_key": {
        "title": "Sse Customer Key",
        "help": "To use SSE-C, the optional header that specifies the base64-encoded 256-bit encryption key to use to\nencrypt or  decrypt the data. Please note only one of sse_customer_key_file|sse_customer_key|sse_kms_key_id is\nneeded. For more information, see Using Your Own Keys for Server-Side Encryption \n(https://docs.cloud.oracle.com/Content/Object/Tasks/usingyourencryptionkeys.htm)"
      },
      "sse_customer_key_file": {
        "title": "Sse Customer Key File",
        "help": "To use SSE-C, a file containing the base64-encoded string of the AES-256 encryption key associated\nwith the object. Please note only one of sse_customer_key_file|sse_customer_key|sse_kms_key_id is needed.'"
      },
      "sse_customer_key_sha256": {
        "title": "Sse Customer Key Sha256",
        "help": "If using SSE-C, The optional header that specifies the base64-encoded SHA256 hash of the encryption\nkey. This value is used to check the integrity of the encryption key. see Using Your Own Keys for \nServer-Side Encryption (https://docs.cloud.oracle.com/Content/Object/Tasks/usingyourencryptionkeys.htm)."
      },
      "sse_kms_key_id": {
        "title": "Sse Kms Key Id",
        "help": "if using your own master key in vault, this header specifies the\nOCID (https://docs.cloud.oracle.com/Content/General/Concepts/identifiers.htm) of a master encryption key used to call\nthe Key Management service to generate a data encryption key or to encrypt or decrypt a data encryption key.\nPlease note only one of sse_customer_key_file|sse_customer_key|sse_kms_key_id is needed."
      },
      "storage_tier": {
        "title": "Storage Tier",
        "help": "The storage class to use when storing new objects in storage. https://docs.oracle.com/en-us/iaas/Content/Object/Concepts/understandingstoragetiers.htm"
      },
      "upload_concurrency": {
        "title": "Upload Concurrency",
        "help": "Concurrency for multipart uploads.\n\nThis is the number of chunks of the same file that are uploaded\nconcurrently.\n\nIf you are uploading small numbers of large files over high-speed links\nand these uploads do not fully utilize your bandwidth, then increasing\nthis may help to speed up the transfers."
      },
      "upload_cutoff": {
        "title": "Upload Cutoff",
        "help": "Cutoff for switching to chunked upload.\n\nAny files larger than this will be uploaded in chunks of chunk_size.\nThe minimum is 0 and the maximum is 5 GiB."
      }
    },
    "pcloud": {
      "auth_url": {
        "title": "Auth Url",
        "help": "Auth server URL.\n\nLeave blank to use the provider defaults."
      },
      "client_credentials": {
        "title": "Client Credentials",
        "help": "Use client credentials OAuth flow.\n\nThis will use the OAUTH2 client Credentials Flow as described in RFC 6749.\n\nNote that this option is NOT supported by all backends."
      },
      "client_id": {
        "title": "Client Id",
        "help": "OAuth Client Id.\n\nLeave blank normally."
      },
      "client_secret": {
        "title": "Client Secret",
        "help": "OAuth Client Secret.\n\nLeave blank normally."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "hostname": {
        "title": "Hostname",
        "help": "Hostname to connect to.\n\nThis is normally set when rclone initially does the oauth connection,\nhowever you will need to set it by hand if you are using remote config\nwith rclone authorize.\n"
      },
      "password": {
        "title": "Password",
        "help": "Your pcloud password."
      },
      "root_folder_id": {
        "title": "Root Folder Id",
        "help": "Fill in for rclone to use a non root folder as its starting point."
      },
      "token": {
        "title": "Token",
        "help": "OAuth Access Token as a JSON blob."
      },
      "token_url": {
        "title": "Token Url",
        "help": "Token server url.\n\nLeave blank to use the provider defaults."
      },
      "username": {
        "title": "Username",
        "help": "Your pcloud username.\n\t\t\t\nThis is only required when you want to use the cleanup command. Due to a bug\nin the pcloud API the required API does not support OAuth authentication so\nwe have to rely on user password authentication for it."
      }
    },
    "pikpak": {
      "chunk_size": {
        "title": "Chunk Size",
        "help": "Chunk size for multipart uploads.\n\t\nLarge files will be uploaded in chunks of this size.\n\nNote that this is stored in memory and there may be up to\n\"--transfers\" * \"--pikpak-upload-concurrency\" chunks stored at once\nin memory.\n\nIf you are transferring large files over high-speed links and you have\nenough memory, then increasing this will speed up the transfers.\n\nRclone will automatically increase the chunk size when uploading a\nlarge file of known size to stay below the 10,000 chunks limit.\n\nIncreasing the chunk size decreases the accuracy of the progress\nstatistics displayed with \"-P\" flag."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "device_id": {
        "title": "Device Id",
        "help": "Device ID used for authorization."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "hash_memory_limit": {
        "title": "Hash Memory Limit",
        "help": "Files bigger than this will be cached on disk to calculate hash if required."
      },
      "no_media_link": {
        "title": "No Media Link",
        "help": "Use original file links instead of media links.\n\nThis avoids issues caused by invalid media links, but may reduce download speeds."
      },
      "pass": {
        "title": "Pass",
        "help": "Pikpak password."
      },
      "root_folder_id": {
        "title": "Root Folder Id",
        "help": "ID of the root folder.\nLeave blank normally.\n\nFill in for rclone to use a non root folder as its starting point.\n"
      },
      "trashed_only": {
        "title": "Trashed Only",
        "help": "Only show files that are in the trash.\n\nThis will show trashed files in their original directory structure."
      },
      "upload_concurrency": {
        "title": "Upload Concurrency",
        "help": "Concurrency for multipart uploads.\n\nThis is the number of chunks of the same file that are uploaded\nconcurrently for multipart uploads.\n\nNote that chunks are stored in memory and there may be up to\n\"--transfers\" * \"--pikpak-upload-concurrency\" chunks stored at once\nin memory.\n\nIf you are uploading small numbers of large files over high-speed links\nand these uploads do not fully utilize your bandwidth, then increasing\nthis may help to speed up the transfers."
      },
      "upload_cutoff": {
        "title": "Upload Cutoff",
        "help": "Cutoff for switching to chunked upload.\n\nAny files larger than this will be uploaded in chunks of chunk_size.\nThe minimum is 0 and the maximum is 5 GiB."
      },
      "use_trash": {
        "title": "Use Trash",
        "help": "Send files to the trash instead of deleting permanently.\n\nDefaults to true, namely sending files to the trash.\nUse `--pikpak-use-trash=false` to delete files permanently instead."
      },
      "user": {
        "title": "User",
        "help": "Pikpak username."
      },
      "user_agent": {
        "title": "User Agent",
        "help": "HTTP user agent for pikpak.\n\nDefaults to \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:129.0) Gecko/20100101 Firefox/129.0\" or \"--pikpak-user-agent\" provided on command line."
      }
    },
    "pixeldrain": {
      "api_key": {
        "title": "Api Key",
        "help": "API key for your pixeldrain account.\nFound on https://pixeldrain.com/user/api_keys."
      },
      "api_url": {
        "title": "Api Url",
        "help": "The API endpoint to connect to. In the vast majority of cases it's fine to leave\nthis at default. It is only intended to be changed for testing purposes."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "root_folder_id": {
        "title": "Root Folder Id",
        "help": "Root of the filesystem to use.\n\nSet to 'me' to use your personal filesystem. Set to a shared directory ID to use a shared directory."
      }
    },
    "premiumizeme": {
      "api_key": {
        "title": "Api Key",
        "help": "API Key.\n\nThis is not normally used - use oauth instead.\n"
      },
      "auth_url": {
        "title": "Auth Url",
        "help": "Auth server URL.\n\nLeave blank to use the provider defaults."
      },
      "client_credentials": {
        "title": "Client Credentials",
        "help": "Use client credentials OAuth flow.\n\nThis will use the OAUTH2 client Credentials Flow as described in RFC 6749.\n\nNote that this option is NOT supported by all backends."
      },
      "client_id": {
        "title": "Client Id",
        "help": "OAuth Client Id.\n\nLeave blank normally."
      },
      "client_secret": {
        "title": "Client Secret",
        "help": "OAuth Client Secret.\n\nLeave blank normally."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "token": {
        "title": "Token",
        "help": "OAuth Access Token as a JSON blob."
      },
      "token_url": {
        "title": "Token Url",
        "help": "Token server url.\n\nLeave blank to use the provider defaults."
      }
    },
    "protondrive": {
      "2fa": {
        "title": "2Fa",
        "help": "The 2FA code\n\nThe value can also be provided with --protondrive-2fa=000000\n\nThe 2FA code of your proton drive account if the account is set up with \ntwo-factor authentication"
      },
      "app_version": {
        "title": "App Version",
        "help": "The app version string \n\nThe app version string indicates the client that is currently performing \nthe API request. This information is required and will be sent with every \nAPI request."
      },
      "client_access_token": {
        "title": "Client Access Token",
        "help": "Client access token key (internal use only)"
      },
      "client_refresh_token": {
        "title": "Client Refresh Token",
        "help": "Client refresh token key (internal use only)"
      },
      "client_salted_key_pass": {
        "title": "Client Salted Key Pass",
        "help": "Client salted key pass key (internal use only)"
      },
      "client_uid": {
        "title": "Client Uid",
        "help": "Client uid key (internal use only)"
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "enable_caching": {
        "title": "Enable Caching",
        "help": "Caches the files and folders metadata to reduce API calls\n\nNotice: If you are mounting ProtonDrive as a VFS, please disable this feature, \nas the current implementation doesn't update or clear the cache when there are \nexternal changes. \n\nThe files and folders on ProtonDrive are represented as links with keyrings, \nwhich can be cached to improve performance and be friendly to the API server.\n\nThe cache is currently built for the case when the rclone is the only instance \nperforming operations to the mount point. The event system, which is the proton\nAPI system that provides visibility of what has changed on the drive, is yet \nto be implemented, so updates from other clients won’t be reflected in the \ncache. Thus, if there are concurrent clients accessing the same mount point, \nthen we might have a problem with caching the stale data."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "mailbox_password": {
        "title": "Mailbox Password",
        "help": "The mailbox password of your two-password proton account.\n\nFor more information regarding the mailbox password, please check the \nfollowing official knowledge base article: \nhttps://proton.me/support/the-difference-between-the-mailbox-password-and-login-password\n"
      },
      "original_file_size": {
        "title": "Original File Size",
        "help": "Return the file size before encryption\n\t\t\t\nThe size of the encrypted file will be different from (bigger than) the \noriginal file size. Unless there is a reason to return the file size \nafter encryption is performed, otherwise, set this option to true, as \nfeatures like Open() which will need to be supplied with original content \nsize, will fail to operate properly"
      },
      "otp_secret_key": {
        "title": "Otp Secret Key",
        "help": "The OTP secret key\n\nThe value can also be provided with --protondrive-otp-secret-key=ABCDEFGHIJKLMNOPQRSTUVWXYZ234567\n\nThe OTP secret key of your proton drive account if the account is set up with \ntwo-factor authentication"
      },
      "password": {
        "title": "Password",
        "help": "The password of your proton account."
      },
      "replace_existing_draft": {
        "title": "Replace Existing Draft",
        "help": "Create a new revision when filename conflict is detected\n\nWhen a file upload is cancelled or failed before completion, a draft will be \ncreated and the subsequent upload of the same file to the same location will be \nreported as a conflict.\n\nThe value can also be set by --protondrive-replace-existing-draft=true\n\nIf the option is set to true, the draft will be replaced and then the upload \noperation will restart. If there are other clients also uploading at the same \nfile location at the same time, the behavior is currently unknown. Need to set \nto true for integration tests.\nIf the option is set to false, an error \"a draft exist - usually this means a \nfile is being uploaded at another client, or, there was a failed upload attempt\" \nwill be returned, and no upload will happen."
      },
      "username": {
        "title": "Username",
        "help": "The username of your proton account"
      }
    },
    "putio": {
      "auth_url": {
        "title": "Auth Url",
        "help": "Auth server URL.\n\nLeave blank to use the provider defaults."
      },
      "client_credentials": {
        "title": "Client Credentials",
        "help": "Use client credentials OAuth flow.\n\nThis will use the OAUTH2 client Credentials Flow as described in RFC 6749.\n\nNote that this option is NOT supported by all backends."
      },
      "client_id": {
        "title": "Client Id",
        "help": "OAuth Client Id.\n\nLeave blank normally."
      },
      "client_secret": {
        "title": "Client Secret",
        "help": "OAuth Client Secret.\n\nLeave blank normally."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "token": {
        "title": "Token",
        "help": "OAuth Access Token as a JSON blob."
      },
      "token_url": {
        "title": "Token Url",
        "help": "Token server url.\n\nLeave blank to use the provider defaults."
      }
    },
    "qingstor": {
      "access_key_id": {
        "title": "Access Key Id",
        "help": "QingStor Access Key ID.\n\nLeave blank for anonymous access or runtime credentials."
      },
      "chunk_size": {
        "title": "Chunk Size",
        "help": "Chunk size to use for uploading.\n\nWhen uploading files larger than upload_cutoff they will be uploaded\nas multipart uploads using this chunk size.\n\nNote that \"--qingstor-upload-concurrency\" chunks of this size are buffered\nin memory per transfer.\n\nIf you are transferring large files over high-speed links and you have\nenough memory, then increasing this will speed up the transfers."
      },
      "connection_retries": {
        "title": "Connection Retries",
        "help": "Number of connection retries."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "endpoint": {
        "title": "Endpoint",
        "help": "Enter an endpoint URL to connection QingStor API.\n\nLeave blank will use the default value \"https://qingstor.com:443\"."
      },
      "env_auth": {
        "title": "Env Auth",
        "help": "Get QingStor credentials from runtime.\n\nOnly applies if access_key_id and secret_access_key is blank."
      },
      "secret_access_key": {
        "title": "Secret Access Key",
        "help": "QingStor Secret Access Key (password).\n\nLeave blank for anonymous access or runtime credentials."
      },
      "upload_concurrency": {
        "title": "Upload Concurrency",
        "help": "Concurrency for multipart uploads.\n\nThis is the number of chunks of the same file that are uploaded\nconcurrently.\n\nNB if you set this to > 1 then the checksums of multipart uploads\nbecome corrupted (the uploads themselves are not corrupted though).\n\nIf you are uploading small numbers of large files over high-speed links\nand these uploads do not fully utilize your bandwidth, then increasing\nthis may help to speed up the transfers."
      },
      "upload_cutoff": {
        "title": "Upload Cutoff",
        "help": "Cutoff for switching to chunked upload.\n\nAny files larger than this will be uploaded in chunks of chunk_size.\nThe minimum is 0 and the maximum is 5 GiB."
      },
      "zone": {
        "title": "Zone",
        "help": "Zone to connect to.\n\nDefault is \"pek3a\"."
      }
    },
    "quatrix": {
      "api_key": {
        "title": "Api Key",
        "help": "API key for accessing Quatrix account"
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "effective_upload_time": {
        "title": "Effective Upload Time",
        "help": "Wanted upload time for one chunk"
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "hard_delete": {
        "title": "Hard Delete",
        "help": "Delete files permanently rather than putting them into the trash"
      },
      "host": {
        "title": "Host",
        "help": "Host name of Quatrix account"
      },
      "maximal_summary_chunk_size": {
        "title": "Maximal Summary Chunk Size",
        "help": "The maximal summary for all chunks. It should not be less than 'transfers'*'minimal_chunk_size'"
      },
      "minimal_chunk_size": {
        "title": "Minimal Chunk Size",
        "help": "The minimal size for one chunk"
      },
      "skip_project_folders": {
        "title": "Skip Project Folders",
        "help": "Skip project folders in operations"
      }
    },
    "s3": {
      "access_key_id": {
        "title": "Access Key Id",
        "help": "AWS Access Key ID.\n\nLeave blank for anonymous access or runtime credentials."
      },
      "acl": {
        "title": "Acl",
        "help": "Canned ACL used when creating buckets and storing or copying objects.\n\nThis ACL is used for creating objects and if bucket_acl isn't set, for creating buckets too.\n\nFor more info visit https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl\n\nNote that this ACL is applied when server-side copying objects as S3\ndoesn't copy the ACL from the source but rather writes a fresh one.\n\nIf the acl is an empty string then no X-Amz-Acl: header is added and\nthe default (private) will be used.\n"
      },
      "bucket_acl": {
        "title": "Bucket Acl",
        "help": "Canned ACL used when creating buckets.\n\nFor more info visit https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl\n\nNote that this ACL is applied when only when creating buckets.  If it\nisn't set then \"acl\" is used instead.\n\nIf the \"acl\" and \"bucket_acl\" are empty strings then no X-Amz-Acl:\nheader is added and the default (private) will be used.\n"
      },
      "chunk_size": {
        "title": "Chunk Size",
        "help": "Chunk size to use for uploading.\n\nWhen uploading files larger than upload_cutoff or files with unknown\nsize (e.g. from \"rclone rcat\" or uploaded with \"rclone mount\" or google\nphotos or google docs) they will be uploaded as multipart uploads\nusing this chunk size.\n\nNote that \"--s3-upload-concurrency\" chunks of this size are buffered\nin memory per transfer.\n\nIf you are transferring large files over high-speed links and you have\nenough memory, then increasing this will speed up the transfers.\n\nRclone will automatically increase the chunk size when uploading a\nlarge file of known size to stay below the 10,000 chunks limit.\n\nFiles of unknown size are uploaded with the configured\nchunk_size. Since the default chunk size is 5 MiB and there can be at\nmost 10,000 chunks, this means that by default the maximum size of\na file you can stream upload is 48 GiB.  If you wish to stream upload\nlarger files then you will need to increase chunk_size.\n\nIncreasing the chunk size decreases the accuracy of the progress\nstatistics displayed with \"-P\" flag. Rclone treats chunk as sent when\nit's buffered by the AWS SDK, when in fact it may still be uploading.\nA bigger chunk size means a bigger AWS SDK buffer and progress\nreporting more deviating from the truth.\n"
      },
      "copy_cutoff": {
        "title": "Copy Cutoff",
        "help": "Cutoff for switching to multipart copy.\n\nAny files larger than this that need to be server-side copied will be\ncopied in chunks of this size.\n\nThe minimum is 0 and the maximum is 5 GiB."
      },
      "decompress": {
        "title": "Decompress",
        "help": "If set this will decompress gzip encoded objects.\n\nIt is possible to upload objects to S3 with \"Content-Encoding: gzip\"\nset. Normally rclone will download these files as compressed objects.\n\nIf this flag is set then rclone will decompress these files with\n\"Content-Encoding: gzip\" as they are received. This means that rclone\ncan't check the size and hash but the file contents will be decompressed.\n"
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "directory_bucket": {
        "title": "Directory Bucket",
        "help": "Set to use AWS Directory Buckets\n\nIf you are using an AWS Directory Bucket then set this flag.\n\nThis will ensure no `Content-Md5` headers are sent and ensure `ETag`\nheaders are not interpreted as MD5 sums. `X-Amz-Meta-Md5chksum` will\nbe set on all objects whether single or multipart uploaded.\n\nThis also sets `no_check_bucket = true`.\n\nNote that Directory Buckets do not support:\n\n- Versioning\n- `Content-Encoding: gzip`\n\nRclone limitations with Directory Buckets:\n\n- rclone does not support creating Directory Buckets with `rclone mkdir`\n- ... or removing them with `rclone rmdir` yet\n- Directory Buckets do not appear when doing `rclone lsf` at the top level.\n- Rclone can't remove auto created directories yet. In theory this should\n  work with `directory_markers = true` but it doesn't.\n- Directories don't seem to appear in recursive (ListR) listings.\n"
      },
      "directory_markers": {
        "title": "Directory Markers",
        "help": "Upload an empty object with a trailing slash when a new directory is created\n\nEmpty folders are unsupported for bucket based remotes, this option creates an empty\nobject ending with \"/\", to persist the folder.\n"
      },
      "disable_checksum": {
        "title": "Disable Checksum",
        "help": "Don't store MD5 checksum with object metadata.\n\nNormally rclone will calculate the MD5 checksum of the input before\nuploading it so it can add it to metadata on the object. This is great\nfor data integrity checking but can cause long delays for large files\nto start uploading."
      },
      "disable_http2": {
        "title": "Disable Http2",
        "help": "Disable usage of http2 for S3 backends.\n\nThere is currently an unsolved issue with the s3 (specifically minio) backend\nand HTTP/2.  HTTP/2 is enabled by default for the s3 backend but can be\ndisabled here.  When the issue is solved this flag will be removed.\n\nSee: https://github.com/rclone/rclone/issues/4673, https://github.com/rclone/rclone/issues/3631\n\n"
      },
      "download_url": {
        "title": "Download Url",
        "help": "Custom endpoint for downloads.\nThis is usually set to a CloudFront CDN URL as AWS S3 offers\ncheaper egress for data downloaded through the CloudFront network."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "endpoint": {
        "title": "Endpoint",
        "help": "Endpoint for S3 API.\n\nRequired when using an S3 clone."
      },
      "env_auth": {
        "title": "Env Auth",
        "help": "Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).\n\nOnly applies if access_key_id and secret_access_key is blank."
      },
      "force_path_style": {
        "title": "Force Path Style",
        "help": "If true use path style access if false use virtual hosted style.\n\nIf this is true (the default) then rclone will use path style access,\nif false then rclone will use virtual path style. See [the AWS S3\ndocs](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#access-bucket-intro)\nfor more info.\n\nSome providers (e.g. AWS, Aliyun OSS, Netease COS, or Tencent COS) require this set to\nfalse - rclone will do this automatically based on the provider\nsetting.\n\nNote that if your bucket isn't a valid DNS name, i.e. has '.' or '_' in,\nyou'll need to set this to true.\n"
      },
      "ibm_api_key": {
        "title": "Ibm Api Key",
        "help": "IBM API Key to be used to obtain IAM token"
      },
      "ibm_resource_instance_id": {
        "title": "Ibm Resource Instance Id",
        "help": "IBM service instance id"
      },
      "leave_parts_on_error": {
        "title": "Leave Parts On Error",
        "help": "If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.\n\nIt should be set to true for resuming uploads across different sessions.\n\nWARNING: Storing parts of an incomplete multipart upload counts towards space usage on S3 and will add additional costs if not cleaned up.\n"
      },
      "list_chunk": {
        "title": "List Chunk",
        "help": "Size of listing chunk (response list for each ListObject S3 request).\n\nThis option is also known as \"MaxKeys\", \"max-items\", or \"page-size\" from the AWS S3 specification.\nMost services truncate the response list to 1000 objects even if requested more than that.\nIn AWS S3 this is a global maximum and cannot be changed, see [AWS S3](https://docs.aws.amazon.com/cli/latest/reference/s3/ls.html).\nIn Ceph, this can be increased with the \"rgw list buckets max chunk\" option.\n"
      },
      "list_url_encode": {
        "title": "List Url Encode",
        "help": "Whether to url encode listings: true/false/unset\n\nSome providers support URL encoding listings and where this is\navailable this is more reliable when using control characters in file\nnames. If this is set to unset (the default) then rclone will choose\naccording to the provider setting what to apply, but you can override\nrclone's choice here.\n"
      },
      "list_version": {
        "title": "List Version",
        "help": "Version of ListObjects to use: 1,2 or 0 for auto.\n\nWhen S3 originally launched it only provided the ListObjects call to\nenumerate objects in a bucket.\n\nHowever in May 2016 the ListObjectsV2 call was introduced. This is\nmuch higher performance and should be used if at all possible.\n\nIf set to the default, 0, rclone will guess according to the provider\nset which list objects method to call. If it guesses wrong, then it\nmay be set manually here.\n"
      },
      "location_constraint": {
        "title": "Location Constraint",
        "help": "Location constraint - must be set to match the Region.\n\nLeave blank if not sure. Used when creating buckets only."
      },
      "max_upload_parts": {
        "title": "Max Upload Parts",
        "help": "Maximum number of parts in a multipart upload.\n\nThis option defines the maximum number of multipart chunks to use\nwhen doing a multipart upload.\n\nThis can be useful if a service does not support the AWS S3\nspecification of 10,000 chunks.\n\nRclone will automatically increase the chunk size when uploading a\nlarge file of a known size to stay below this number of chunks limit.\n"
      },
      "memory_pool_flush_time": {
        "title": "Memory Pool Flush Time",
        "help": "How often internal memory buffer pools will be flushed. (no longer used)"
      },
      "memory_pool_use_mmap": {
        "title": "Memory Pool Use Mmap",
        "help": "Whether to use mmap buffers in internal memory pool. (no longer used)"
      },
      "might_gzip": {
        "title": "Might Gzip",
        "help": "Set this if the backend might gzip objects.\n\nNormally providers will not alter objects when they are downloaded. If\nan object was not uploaded with `Content-Encoding: gzip` then it won't\nbe set on download.\n\nHowever some providers may gzip objects even if they weren't uploaded\nwith `Content-Encoding: gzip` (eg Cloudflare).\n\nA symptom of this would be receiving errors like\n\n    ERROR corrupted on transfer: sizes differ NNN vs MMM\n\nIf you set this flag and rclone downloads an object with\nContent-Encoding: gzip set and chunked transfer encoding, then rclone\nwill decompress the object on the fly.\n\nIf this is set to unset (the default) then rclone will choose\naccording to the provider setting what to apply, but you can override\nrclone's choice here.\n"
      },
      "no_check_bucket": {
        "title": "No Check Bucket",
        "help": "If set, don't attempt to check the bucket exists or create it.\n\nThis can be useful when trying to minimise the number of transactions\nrclone does if you know the bucket exists already.\n\nIt can also be needed if the user you are using does not have bucket\ncreation permissions. Before v1.52.0 this would have passed silently\ndue to a bug.\n"
      },
      "no_head": {
        "title": "No Head",
        "help": "If set, don't HEAD uploaded objects to check integrity.\n\nThis can be useful when trying to minimise the number of transactions\nrclone does.\n\nSetting it means that if rclone receives a 200 OK message after\nuploading an object with PUT then it will assume that it got uploaded\nproperly.\n\nIn particular it will assume:\n\n- the metadata, including modtime, storage class and content type was as uploaded\n- the size was as uploaded\n\nIt reads the following items from the response for a single part PUT:\n\n- the MD5SUM\n- The uploaded date\n\nFor multipart uploads these items aren't read.\n\nIf an source object of unknown length is uploaded then rclone **will** do a\nHEAD request.\n\nSetting this flag increases the chance for undetected upload failures,\nin particular an incorrect size, so it isn't recommended for normal\noperation. In practice the chance of an undetected upload failure is\nvery small even with this flag.\n"
      },
      "no_head_object": {
        "title": "No Head Object",
        "help": "If set, do not do HEAD before GET when getting objects."
      },
      "no_system_metadata": {
        "title": "No System Metadata",
        "help": "Suppress setting and reading of system metadata"
      },
      "profile": {
        "title": "Profile",
        "help": "Profile to use in the shared credentials file.\n\nIf env_auth = true then rclone can use a shared credentials file. This\nvariable controls which profile is used in that file.\n\nIf empty it will default to the environment variable \"AWS_PROFILE\" or\n\"default\" if that environment variable is also not set.\n"
      },
      "provider": {
        "title": "Provider",
        "help": "Choose your S3 provider."
      },
      "region": {
        "title": "Region",
        "help": "Region to connect to.\n\nLeave blank if you are using an S3 clone and you don't have a region."
      },
      "requester_pays": {
        "title": "Requester Pays",
        "help": "Enables requester pays option when interacting with S3 bucket."
      },
      "sdk_log_mode": {
        "title": "Sdk Log Mode",
        "help": "Set to debug the SDK\n\nThis can be set to a comma separated list of the following functions:\n\n- `Signing`\n- `Retries`\n- `Request`\n- `RequestWithBody`\n- `Response`\n- `ResponseWithBody`\n- `DeprecatedUsage`\n- `RequestEventMessage`\n- `ResponseEventMessage`\n\nUse `Off` to disable and `All` to set all log levels. You will need to\nuse `-vv` to see the debug level logs.\n"
      },
      "secret_access_key": {
        "title": "Secret Access Key",
        "help": "AWS Secret Access Key (password).\n\nLeave blank for anonymous access or runtime credentials."
      },
      "server_side_encryption": {
        "title": "Server Side Encryption",
        "help": "The server-side encryption algorithm used when storing this object in S3."
      },
      "session_token": {
        "title": "Session Token",
        "help": "An AWS session token."
      },
      "shared_credentials_file": {
        "title": "Shared Credentials File",
        "help": "Path to the shared credentials file.\n\nIf env_auth = true then rclone can use a shared credentials file.\n\nIf this variable is empty rclone will look for the\n\"AWS_SHARED_CREDENTIALS_FILE\" env variable. If the env value is empty\nit will default to the current user's home directory.\n\n    Linux/OSX: \"$HOME/.aws/credentials\"\n    Windows:   \"%USERPROFILE%\\.aws\\credentials\"\n"
      },
      "sign_accept_encoding": {
        "title": "Sign Accept Encoding",
        "help": "Set if rclone should include Accept-Encoding as part of the signature.\n\nYou can change this if you want to stop rclone including\nAccept-Encoding as part of the signature.\n\nThis shouldn't be necessary in normal operation.\n\nThis should be automatically set correctly for all providers rclone\nknows about - please make a bug report if not.\n"
      },
      "sse_customer_algorithm": {
        "title": "Sse Customer Algorithm",
        "help": "If using SSE-C, the server-side encryption algorithm used when storing this object in S3."
      },
      "sse_customer_key": {
        "title": "Sse Customer Key",
        "help": "To use SSE-C you may provide the secret encryption key used to encrypt/decrypt your data.\n\nAlternatively you can provide --sse-customer-key-base64."
      },
      "sse_customer_key_base64": {
        "title": "Sse Customer Key Base64",
        "help": "If using SSE-C you must provide the secret encryption key encoded in base64 format to encrypt/decrypt your data.\n\nAlternatively you can provide --sse-customer-key."
      },
      "sse_customer_key_md5": {
        "title": "Sse Customer Key Md5",
        "help": "If using SSE-C you may provide the secret encryption key MD5 checksum (optional).\n\nIf you leave it blank, this is calculated automatically from the sse_customer_key provided.\n"
      },
      "sse_kms_key_id": {
        "title": "Sse Kms Key Id",
        "help": "If using KMS ID you must provide the ARN of Key."
      },
      "storage_class": {
        "title": "Storage Class",
        "help": "The storage class to use when storing new objects in S3."
      },
      "sts_endpoint": {
        "title": "Sts Endpoint",
        "help": "Endpoint for STS (deprecated).\n\nLeave blank if using AWS to use the default endpoint for the region."
      },
      "upload_concurrency": {
        "title": "Upload Concurrency",
        "help": "Concurrency for multipart uploads and copies.\n\nThis is the number of chunks of the same file that are uploaded\nconcurrently for multipart uploads and copies.\n\nIf you are uploading small numbers of large files over high-speed links\nand these uploads do not fully utilize your bandwidth, then increasing\nthis may help to speed up the transfers."
      },
      "upload_cutoff": {
        "title": "Upload Cutoff",
        "help": "Cutoff for switching to chunked upload.\n\nAny files larger than this will be uploaded in chunks of chunk_size.\nThe minimum is 0 and the maximum is 5 GiB."
      },
      "use_accelerate_endpoint": {
        "title": "Use Accelerate Endpoint",
        "help": "If true use the AWS S3 accelerated endpoint.\n\nSee: [AWS S3 Transfer acceleration](https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration-examples.html)"
      },
      "use_accept_encoding_gzip": {
        "title": "Use Accept Encoding Gzip",
        "help": "Whether to send `Accept-Encoding: gzip` header.\n\nBy default, rclone will append `Accept-Encoding: gzip` to the request to download\ncompressed objects whenever possible.\n\nHowever some providers such as Google Cloud Storage may alter the HTTP headers, breaking\nthe signature of the request.\n\nA symptom of this would be receiving errors like\n\n\tSignatureDoesNotMatch: The request signature we calculated does not match the signature you provided.\n\nIn this case, you might want to try disabling this option.\n"
      },
      "use_already_exists": {
        "title": "Use Already Exists",
        "help": "Set if rclone should report BucketAlreadyExists errors on bucket creation.\n\nAt some point during the evolution of the s3 protocol, AWS started\nreturning an `AlreadyOwnedByYou` error when attempting to create a\nbucket that the user already owned, rather than a\n`BucketAlreadyExists` error.\n\nUnfortunately exactly what has been implemented by s3 clones is a\nlittle inconsistent, some return `AlreadyOwnedByYou`, some return\n`BucketAlreadyExists` and some return no error at all.\n\nThis is important to rclone because it ensures the bucket exists by\ncreating it on quite a lot of operations (unless\n`--s3-no-check-bucket` is used).\n\nIf rclone knows the provider can return `AlreadyOwnedByYou` or returns\nno error then it can report `BucketAlreadyExists` errors when the user\nattempts to create a bucket not owned by them. Otherwise rclone\nignores the `BucketAlreadyExists` error which can lead to confusion.\n\nThis should be automatically set correctly for all providers rclone\nknows about - please make a bug report if not.\n"
      },
      "use_arn_region": {
        "title": "Use Arn Region",
        "help": "If true, enables arn region support for the service."
      },
      "use_data_integrity_protections": {
        "title": "Use Data Integrity Protections",
        "help": "If true use AWS S3 data integrity protections.\n\nSee [AWS Docs on Data Integrity Protections](https://docs.aws.amazon.com/sdkref/latest/guide/feature-dataintegrity.html)"
      },
      "use_dual_stack": {
        "title": "Use Dual Stack",
        "help": "If true use AWS S3 dual-stack endpoint (IPv6 support).\n\nSee [AWS Docs on Dualstack Endpoints](https://docs.aws.amazon.com/AmazonS3/latest/userguide/dual-stack-endpoints.html)"
      },
      "use_multipart_etag": {
        "title": "Use Multipart Etag",
        "help": "Whether to use ETag in multipart uploads for verification\n\nThis should be true, false or left unset to use the default for the provider.\n"
      },
      "use_multipart_uploads": {
        "title": "Use Multipart Uploads",
        "help": "Set if rclone should use multipart uploads.\n\nYou can change this if you want to disable the use of multipart uploads.\nThis shouldn't be necessary in normal operation.\n\nThis should be automatically set correctly for all providers rclone\nknows about - please make a bug report if not.\n"
      },
      "use_presigned_request": {
        "title": "Use Presigned Request",
        "help": "Whether to use a presigned request or PutObject for single part uploads\n\nIf this is false rclone will use PutObject from the AWS SDK to upload\nan object.\n\nVersions of rclone < 1.59 use presigned requests to upload a single\npart object and setting this flag to true will re-enable that\nfunctionality. This shouldn't be necessary except in exceptional\ncircumstances or for testing.\n"
      },
      "use_unsigned_payload": {
        "title": "Use Unsigned Payload",
        "help": "Whether to use an unsigned payload in PutObject\n\nRclone has to avoid the AWS SDK seeking the body when calling\nPutObject. The AWS provider can add checksums in the trailer to avoid\nseeking but other providers can't.\n\nThis should be true, false or left unset to use the default for the provider.\n"
      },
      "use_x_id": {
        "title": "Use X Id",
        "help": "Set if rclone should add x-id URL parameters.\n\nYou can change this if you want to disable the AWS SDK from\nadding x-id URL parameters.\n\nThis shouldn't be necessary in normal operation.\n\nThis should be automatically set correctly for all providers rclone\nknows about - please make a bug report if not.\n"
      },
      "v2_auth": {
        "title": "V2 Auth",
        "help": "If true use v2 authentication.\n\nIf this is false (the default) then rclone will use v4 authentication.\nIf it is set then rclone will use v2 authentication.\n\nUse this only if v4 signatures don't work, e.g. pre Jewel/v10 CEPH."
      },
      "version_at": {
        "title": "Version At",
        "help": "Show file versions as they were at the specified time.\n\nThe parameter should be a date, \"2006-01-02\", datetime \"2006-01-02\n15:04:05\" or a duration for that long ago, eg \"100d\" or \"1h\".\n\nNote that when using this no file write operations are permitted,\nso you can't upload files or delete them.\n\nSee [the time option docs](/docs/#time-options) for valid formats.\n"
      },
      "version_deleted": {
        "title": "Version Deleted",
        "help": "Show deleted file markers when using versions.\n\nThis shows deleted file markers in the listing when using versions. These will appear\nas 0 size files. The only operation which can be performed on them is deletion.\n\nDeleting a delete marker will reveal the previous version.\n\nDeleted files will always show with a timestamp.\n"
      },
      "versions": {
        "title": "Versions",
        "help": "Include old versions in directory listings."
      }
    },
    "seafile": {
      "2fa": {
        "title": "2Fa",
        "help": "Two-factor authentication ('true' if the account has 2FA enabled)."
      },
      "auth_token": {
        "title": "Auth Token",
        "help": "Authentication token."
      },
      "create_library": {
        "title": "Create Library",
        "help": "Should rclone create a library if it doesn't exist."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "library": {
        "title": "Library",
        "help": "Name of the library.\n\nLeave blank to access all non-encrypted libraries."
      },
      "library_key": {
        "title": "Library Key",
        "help": "Library password (for encrypted libraries only).\n\nLeave blank if you pass it through the command line."
      },
      "pass": {
        "title": "Pass",
        "help": "Password."
      },
      "url": {
        "title": "Url",
        "help": "URL of seafile host to connect to."
      },
      "user": {
        "title": "User",
        "help": "User name (usually email address)."
      }
    },
    "sftp": {
      "ask_password": {
        "title": "Ask Password",
        "help": "Allow asking for SFTP password when needed.\n\nIf this is set and no password is supplied then rclone will:\n- ask for a password\n- not contact the ssh agent\n"
      },
      "blake3sum_command": {
        "title": "Blake3Sum Command",
        "help": "The command used to read BLAKE3 hashes.\n\nLeave blank for autodetect."
      },
      "chunk_size": {
        "title": "Chunk Size",
        "help": "Upload and download chunk size.\n\nThis controls the maximum size of payload in SFTP protocol packets.\nThe RFC limits this to 32768 bytes (32k), which is the default. However,\na lot of servers support larger sizes, typically limited to a maximum\ntotal package size of 256k, and setting it larger will increase transfer\nspeed dramatically on high latency links. This includes OpenSSH, and,\nfor example, using the value of 255k works well, leaving plenty of room\nfor overhead while still being within a total packet size of 256k.\n\nMake sure to test thoroughly before using a value higher than 32k,\nand only use it if you always connect to the same server or after\nsufficiently broad testing. If you get errors such as\n\"failed to send packet payload: EOF\", lots of \"connection lost\",\nor \"corrupted on transfer\", when copying a larger file, try lowering\nthe value. The server run by [rclone serve sftp](/commands/rclone_serve_sftp)\nsends packets with standard 32k maximum payload so you must not\nset a different chunk_size when downloading files, but it accepts\npackets up to the 256k total size, so for uploads the chunk_size\ncan be set as for the OpenSSH example above.\n"
      },
      "ciphers": {
        "title": "Ciphers",
        "help": "Space separated list of ciphers to be used for session encryption, ordered by preference.\n\nAt least one must match with server configuration. This can be checked for example using ssh -Q cipher.\n\nThis must not be set if use_insecure_cipher is true.\n\nExample:\n\n    aes128-ctr aes192-ctr aes256-ctr aes128-gcm@openssh.com aes256-gcm@openssh.com\n"
      },
      "concurrency": {
        "title": "Concurrency",
        "help": "The maximum number of outstanding requests for one file\n\nThis controls the maximum number of outstanding requests for one file.\nIncreasing it will increase throughput on high latency links at the\ncost of using more memory.\n"
      },
      "connections": {
        "title": "Connections",
        "help": "Maximum number of SFTP simultaneous connections, 0 for unlimited.\n\nNote that setting this is very likely to cause deadlocks so it should\nbe used with care.\n\nIf you are doing a sync or copy then make sure connections is one more\nthan the sum of `--transfers` and `--checkers`.\n\nIf you use `--check-first` then it just needs to be one more than the\nmaximum of `--checkers` and `--transfers`.\n\nSo for `connections 3` you'd use `--checkers 2 --transfers 2\n--check-first` or `--checkers 1 --transfers 1`.\n\n"
      },
      "copy_is_hardlink": {
        "title": "Copy Is Hardlink",
        "help": "Set to enable server side copies using hardlinks.\n\nThe SFTP protocol does not define a copy command so normally server\nside copies are not allowed with the sftp backend.\n\nHowever the SFTP protocol does support hardlinking, and if you enable\nthis flag then the sftp backend will support server side copies. These\nwill be implemented by doing a hardlink from the source to the\ndestination.\n\nNot all sftp servers support this.\n\nNote that hardlinking two files together will use no additional space\nas the source and the destination will be the same file.\n\nThis feature may be useful backups made with --copy-dest."
      },
      "crc32sum_command": {
        "title": "Crc32Sum Command",
        "help": "The command used to read CRC-32 hashes.\n\nLeave blank for autodetect."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "disable_concurrent_reads": {
        "title": "Disable Concurrent Reads",
        "help": "If set don't use concurrent reads.\n\nNormally concurrent reads are safe to use and not using them will\ndegrade performance, so this option is disabled by default.\n\nSome servers limit the amount number of times a file can be\ndownloaded. Using concurrent reads can trigger this limit, so if you\nhave a server which returns\n\n    Failed to copy: file does not exist\n\nThen you may need to enable this flag.\n\nIf concurrent reads are disabled, the use_fstat option is ignored.\n"
      },
      "disable_concurrent_writes": {
        "title": "Disable Concurrent Writes",
        "help": "If set don't use concurrent writes.\n\nNormally rclone uses concurrent writes to upload files. This improves\nthe performance greatly, especially for distant servers.\n\nThis option disables concurrent writes should that be necessary.\n"
      },
      "disable_hashcheck": {
        "title": "Disable Hashcheck",
        "help": "Disable the execution of SSH commands to determine if remote file hashing is available.\n\nLeave blank or set to false to enable hashing (recommended), set to true to disable hashing."
      },
      "hashes": {
        "title": "Hashes",
        "help": "Comma separated list of supported checksum types."
      },
      "host": {
        "title": "Host",
        "help": "SSH host to connect to.\n\nE.g. \"example.com\"."
      },
      "host_key_algorithms": {
        "title": "Host Key Algorithms",
        "help": "Space separated list of host key algorithms, ordered by preference.\n\nAt least one must match with server configuration. This can be checked for example using ssh -Q HostKeyAlgorithms.\n\nNote: This can affect the outcome of key negotiation with the server even if server host key validation is not enabled.\n\nExample:\n\n    ssh-ed25519 ssh-rsa ssh-dss\n"
      },
      "http_proxy": {
        "title": "Http Proxy",
        "help": "URL for HTTP CONNECT proxy\n\nSet this to a URL for an HTTP proxy which supports the HTTP CONNECT verb.\n"
      },
      "idle_timeout": {
        "title": "Idle Timeout",
        "help": "Max time before closing idle connections.\n\nIf no connections have been returned to the connection pool in the time\ngiven, rclone will empty the connection pool.\n\nSet to 0 to keep connections indefinitely.\n"
      },
      "key_exchange": {
        "title": "Key Exchange",
        "help": "Space separated list of key exchange algorithms, ordered by preference.\n\nAt least one must match with server configuration. This can be checked for example using ssh -Q kex.\n\nThis must not be set if use_insecure_cipher is true.\n\nExample:\n\n    sntrup761x25519-sha512@openssh.com curve25519-sha256 curve25519-sha256@libssh.org ecdh-sha2-nistp256\n"
      },
      "key_file": {
        "title": "Key File",
        "help": "Path to PEM-encoded private key file.\n\nLeave blank or set key-use-agent to use ssh-agent.\n\nLeading `~` will be expanded in the file name as will environment variables such as `${RCLONE_CONFIG_DIR}`."
      },
      "key_file_pass": {
        "title": "Key File Pass",
        "help": "The passphrase to decrypt the PEM-encoded private key file.\n\nOnly PEM encrypted key files (old OpenSSH format) are supported. Encrypted keys\nin the new OpenSSH format can't be used."
      },
      "key_pem": {
        "title": "Key Pem",
        "help": "Raw PEM-encoded private key.\n\nNote that this should be on a single line with line endings replaced with '\\n', eg\n\n    key_pem = -----BEGIN RSA PRIVATE KEY-----\\nMaMbaIXtE\\n0gAMbMbaSsd\\nMbaass\\n-----END RSA PRIVATE KEY-----\n\nThis will generate the single line correctly:\n\n    awk '{printf \"%s\\\\n\", $0}' < ~/.ssh/id_rsa\n\nIf specified, it will override the key_file parameter."
      },
      "key_use_agent": {
        "title": "Key Use Agent",
        "help": "When set forces the usage of the ssh-agent.\n\nWhen key-file is also set, the \".pub\" file of the specified key-file is read and only the associated key is\nrequested from the ssh-agent. This allows to avoid `Too many authentication failures for *username*` errors\nwhen the ssh-agent contains many keys."
      },
      "known_hosts_file": {
        "title": "Known Hosts File",
        "help": "Optional path to known_hosts file.\n\nSet this value to enable server host key validation.\n\nLeading `~` will be expanded in the file name as will environment variables such as `${RCLONE_CONFIG_DIR}`."
      },
      "macs": {
        "title": "Macs",
        "help": "Space separated list of MACs (message authentication code) algorithms, ordered by preference.\n\nAt least one must match with server configuration. This can be checked for example using ssh -Q mac.\n\nExample:\n\n    umac-64-etm@openssh.com umac-128-etm@openssh.com hmac-sha2-256-etm@openssh.com\n"
      },
      "md5sum_command": {
        "title": "Md5Sum Command",
        "help": "The command used to read MD5 hashes.\n\nLeave blank for autodetect."
      },
      "pass": {
        "title": "Pass",
        "help": "SSH password, leave blank to use ssh-agent."
      },
      "path_override": {
        "title": "Path Override",
        "help": "Override path used by SSH shell commands.\n\nThis allows checksum calculation when SFTP and SSH paths are\ndifferent. This issue affects among others Synology NAS boxes.\n\nE.g. if shared folders can be found in directories representing volumes:\n\n    rclone sync /home/local/directory remote:/directory --sftp-path-override /volume2/directory\n\nE.g. if home directory can be found in a shared folder called \"home\":\n\n    rclone sync /home/local/directory remote:/home/directory --sftp-path-override /volume1/homes/USER/directory\n\t\nTo specify only the path to the SFTP remote's root, and allow rclone to add any relative subpaths automatically (including unwrapping/decrypting remotes as necessary), add the '@' character to the beginning of the path.\n\nE.g. the first example above could be rewritten as:\n\n\trclone sync /home/local/directory remote:/directory --sftp-path-override @/volume2\n\t\nNote that when using this method with Synology \"home\" folders, the full \"/homes/USER\" path should be specified instead of \"/home\".\n\nE.g. the second example above should be rewritten as:\n\n\trclone sync /home/local/directory remote:/homes/USER/directory --sftp-path-override @/volume1"
      },
      "port": {
        "title": "Port",
        "help": "SSH port number."
      },
      "pubkey": {
        "title": "Pubkey",
        "help": "SSH public certificate for public certificate based authentication.\nSet this if you have a signed certificate you want to use for authentication.\nIf specified will override pubkey_file."
      },
      "pubkey_file": {
        "title": "Pubkey File",
        "help": "Optional path to public key file.\n\nSet this if you have a signed certificate you want to use for authentication.\n\nLeading `~` will be expanded in the file name as will environment variables such as `${RCLONE_CONFIG_DIR}`."
      },
      "server_command": {
        "title": "Server Command",
        "help": "Specifies the path or command to run a sftp server on the remote host.\n\nThe subsystem option is ignored when server_command is defined.\n\nIf adding server_command to the configuration file please note that \nit should not be enclosed in quotes, since that will make rclone fail.\n\nA working example is:\n\n    [remote_name]\n    type = sftp\n    server_command = sudo /usr/libexec/openssh/sftp-server"
      },
      "set_env": {
        "title": "Set Env",
        "help": "Environment variables to pass to sftp and commands\n\nSet environment variables in the form:\n\n    VAR=value\n\nto be passed to the sftp client and to any commands run (eg md5sum).\n\nPass multiple variables space separated, eg\n\n    VAR1=value VAR2=value\n\nand pass variables with spaces in quotes, eg\n\n    \"VAR3=value with space\" \"VAR4=value with space\" VAR5=nospacehere\n\n"
      },
      "set_modtime": {
        "title": "Set Modtime",
        "help": "Set the modified time on the remote if set."
      },
      "sha1sum_command": {
        "title": "Sha1Sum Command",
        "help": "The command used to read SHA-1 hashes.\n\nLeave blank for autodetect."
      },
      "sha256sum_command": {
        "title": "Sha256Sum Command",
        "help": "The command used to read SHA-256 hashes.\n\nLeave blank for autodetect."
      },
      "shell_type": {
        "title": "Shell Type",
        "help": "The type of SSH shell on remote server, if any.\n\nLeave blank for autodetect."
      },
      "skip_links": {
        "title": "Skip Links",
        "help": "Set to skip any symlinks and any other non regular files."
      },
      "socks_proxy": {
        "title": "Socks Proxy",
        "help": "Socks 5 proxy host.\n\t\nSupports the format user:pass@host:port, user@host:port, host:port.\n\nExample:\n\n\tmyUser:myPass@localhost:9005\n\t"
      },
      "ssh": {
        "title": "Ssh",
        "help": "Path and arguments to external ssh binary.\n\nNormally rclone will use its internal ssh library to connect to the\nSFTP server. However it does not implement all possible ssh options so\nit may be desirable to use an external ssh binary.\n\nRclone ignores all the internal config if you use this option and\nexpects you to configure the ssh binary with the user/host/port and\nany other options you need.\n\n**Important** The ssh command must log in without asking for a\npassword so needs to be configured with keys or certificates.\n\nRclone will run the command supplied either with the additional\narguments \"-s sftp\" to access the SFTP subsystem or with commands such\nas \"md5sum /path/to/file\" appended to read checksums.\n\nAny arguments with spaces in should be surrounded by \"double quotes\".\n\nAn example setting might be:\n\n    ssh -o ServerAliveInterval=20 user@example.com\n\nNote that when using an external ssh binary rclone makes a new ssh\nconnection for every hash it calculates.\n"
      },
      "subsystem": {
        "title": "Subsystem",
        "help": "Specifies the SSH2 subsystem on the remote host."
      },
      "use_fstat": {
        "title": "Use Fstat",
        "help": "If set use fstat instead of stat.\n\nSome servers limit the amount of open files and calling Stat after opening\nthe file will throw an error from the server. Setting this flag will call\nFstat instead of Stat which is called on an already open file handle.\n\nIt has been found that this helps with IBM Sterling SFTP servers which have\n\"extractability\" level set to 1 which means only 1 file can be opened at\nany given time.\n"
      },
      "use_insecure_cipher": {
        "title": "Use Insecure Cipher",
        "help": "Enable the use of insecure ciphers and key exchange methods.\n\nThis enables the use of the following insecure ciphers and key exchange methods:\n\n- aes128-cbc\n- aes192-cbc\n- aes256-cbc\n- 3des-cbc\n- diffie-hellman-group-exchange-sha256\n- diffie-hellman-group-exchange-sha1\n\nThose algorithms are insecure and may allow plaintext data to be recovered by an attacker.\n\nThis must be false if you use either ciphers or key_exchange advanced options.\n"
      },
      "user": {
        "title": "User",
        "help": "SSH username."
      },
      "xxh128sum_command": {
        "title": "Xxh128Sum Command",
        "help": "The command used to read XXH128 hashes.\n\nLeave blank for autodetect."
      },
      "xxh3sum_command": {
        "title": "Xxh3Sum Command",
        "help": "The command used to read XXH3 hashes.\n\nLeave blank for autodetect."
      }
    },
    "sharefile": {
      "auth_url": {
        "title": "Auth Url",
        "help": "Auth server URL.\n\nLeave blank to use the provider defaults."
      },
      "chunk_size": {
        "title": "Chunk Size",
        "help": "Upload chunk size.\n\nMust a power of 2 >= 256k.\n\nMaking this larger will improve performance, but note that each chunk\nis buffered in memory one per transfer.\n\nReducing this will reduce memory usage but decrease performance."
      },
      "client_credentials": {
        "title": "Client Credentials",
        "help": "Use client credentials OAuth flow.\n\nThis will use the OAUTH2 client Credentials Flow as described in RFC 6749.\n\nNote that this option is NOT supported by all backends."
      },
      "client_id": {
        "title": "Client Id",
        "help": "OAuth Client Id.\n\nLeave blank normally."
      },
      "client_secret": {
        "title": "Client Secret",
        "help": "OAuth Client Secret.\n\nLeave blank normally."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "endpoint": {
        "title": "Endpoint",
        "help": "Endpoint for API calls.\n\nThis is usually auto discovered as part of the oauth process, but can\nbe set manually to something like: https://XXX.sharefile.com\n"
      },
      "root_folder_id": {
        "title": "Root Folder Id",
        "help": "ID of the root folder.\n\nLeave blank to access \"Personal Folders\".  You can use one of the\nstandard values here or any folder ID (long hex number ID)."
      },
      "token": {
        "title": "Token",
        "help": "OAuth Access Token as a JSON blob."
      },
      "token_url": {
        "title": "Token Url",
        "help": "Token server url.\n\nLeave blank to use the provider defaults."
      },
      "upload_cutoff": {
        "title": "Upload Cutoff",
        "help": "Cutoff for switching to multipart upload."
      }
    },
    "sia": {
      "api_password": {
        "title": "Api Password",
        "help": "Sia Daemon API Password.\n\nCan be found in the apipassword file located in HOME/.sia/ or in the daemon directory."
      },
      "api_url": {
        "title": "Api Url",
        "help": "Sia daemon API URL, like http://sia.daemon.host:9980.\n\nNote that siad must run with --disable-api-security to open API port for other hosts (not recommended).\nKeep default if Sia daemon runs on localhost."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "user_agent": {
        "title": "User Agent",
        "help": "Siad User Agent\n\nSia daemon requires the 'Sia-Agent' user agent by default for security"
      }
    },
    "smb": {
      "case_insensitive": {
        "title": "Case Insensitive",
        "help": "Whether the server is configured to be case-insensitive.\n\nAlways true on Windows shares."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "domain": {
        "title": "Domain",
        "help": "Domain name for NTLM authentication."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "hide_special_share": {
        "title": "Hide Special Share",
        "help": "Hide special shares (e.g. print$) which users aren't supposed to access."
      },
      "host": {
        "title": "Host",
        "help": "SMB server hostname to connect to.\n\nE.g. \"example.com\"."
      },
      "idle_timeout": {
        "title": "Idle Timeout",
        "help": "Max time before closing idle connections.\n\nIf no connections have been returned to the connection pool in the time\ngiven, rclone will empty the connection pool.\n\nSet to 0 to keep connections indefinitely.\n"
      },
      "kerberos_ccache": {
        "title": "Kerberos Ccache",
        "help": "Path to the Kerberos credential cache (krb5cc).\n\nOverrides the default KRB5CCNAME environment variable and allows this\ninstance of the SMB backend to use a different Kerberos cache file.\nThis is useful when mounting multiple SMB with different credentials\nor running in multi-user environments.\n\nSupported formats:\n  - FILE:/path/to/ccache   – Use the specified file.\n  - DIR:/path/to/ccachedir – Use the primary file inside the specified directory.\n  - /path/to/ccache        – Interpreted as a file path."
      },
      "pass": {
        "title": "Pass",
        "help": "SMB password."
      },
      "port": {
        "title": "Port",
        "help": "SMB port number."
      },
      "spn": {
        "title": "Spn",
        "help": "Service principal name.\n\nRclone presents this name to the server. Some servers use this as further\nauthentication, and it often needs to be set for clusters. For example:\n\n    cifs/remotehost:1020\n\nLeave blank if not sure.\n"
      },
      "use_kerberos": {
        "title": "Use Kerberos",
        "help": "Use Kerberos authentication.\n\nIf set, rclone will use Kerberos authentication instead of NTLM. This\nrequires a valid Kerberos configuration and credentials cache to be\navailable, either in the default locations or as specified by the\nKRB5_CONFIG and KRB5CCNAME environment variables.\n"
      },
      "user": {
        "title": "User",
        "help": "SMB username."
      }
    },
    "storj": {
      "access_grant": {
        "title": "Access Grant",
        "help": "Access grant."
      },
      "api_key": {
        "title": "Api Key",
        "help": "API key."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "passphrase": {
        "title": "Passphrase",
        "help": "Encryption passphrase.\n\nTo access existing objects enter passphrase used for uploading."
      },
      "provider": {
        "title": "Provider",
        "help": "Choose an authentication method."
      },
      "satellite_address": {
        "title": "Satellite Address",
        "help": "Satellite address.\n\nCustom satellite address should match the format: `<nodeid>@<address>:<port>`."
      }
    },
    "sugarsync": {
      "access_key_id": {
        "title": "Access Key Id",
        "help": "Sugarsync Access Key ID.\n\nLeave blank to use rclone's."
      },
      "app_id": {
        "title": "App Id",
        "help": "Sugarsync App ID.\n\nLeave blank to use rclone's."
      },
      "authorization": {
        "title": "Authorization",
        "help": "Sugarsync authorization.\n\nLeave blank normally, will be auto configured by rclone."
      },
      "authorization_expiry": {
        "title": "Authorization Expiry",
        "help": "Sugarsync authorization expiry.\n\nLeave blank normally, will be auto configured by rclone."
      },
      "deleted_id": {
        "title": "Deleted Id",
        "help": "Sugarsync deleted folder id.\n\nLeave blank normally, will be auto configured by rclone."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "hard_delete": {
        "title": "Hard Delete",
        "help": "Permanently delete files if true\notherwise put them in the deleted files."
      },
      "private_access_key": {
        "title": "Private Access Key",
        "help": "Sugarsync Private Access Key.\n\nLeave blank to use rclone's."
      },
      "refresh_token": {
        "title": "Refresh Token",
        "help": "Sugarsync refresh token.\n\nLeave blank normally, will be auto configured by rclone."
      },
      "root_id": {
        "title": "Root Id",
        "help": "Sugarsync root id.\n\nLeave blank normally, will be auto configured by rclone."
      },
      "user": {
        "title": "User",
        "help": "Sugarsync user.\n\nLeave blank normally, will be auto configured by rclone."
      }
    },
    "swift": {
      "application_credential_id": {
        "title": "Application Credential Id",
        "help": "Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)."
      },
      "application_credential_name": {
        "title": "Application Credential Name",
        "help": "Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)."
      },
      "application_credential_secret": {
        "title": "Application Credential Secret",
        "help": "Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)."
      },
      "auth": {
        "title": "Auth",
        "help": "Authentication URL for server (OS_AUTH_URL)."
      },
      "auth_token": {
        "title": "Auth Token",
        "help": "Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)."
      },
      "auth_version": {
        "title": "Auth Version",
        "help": "AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)."
      },
      "chunk_size": {
        "title": "Chunk Size",
        "help": "Above this size files will be chunked.\n\nAbove this size files will be chunked into a a `_segments` container\nor a `.file-segments` directory. (See the `use_segments_container` option\nfor more info). Default for this is 5 GiB which is its maximum value, which\nmeans only files above this size will be chunked.\n\nRclone uploads chunked files as dynamic large objects (DLO).\n"
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "domain": {
        "title": "Domain",
        "help": "User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)"
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "endpoint_type": {
        "title": "Endpoint Type",
        "help": "Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)."
      },
      "env_auth": {
        "title": "Env Auth",
        "help": "Get swift credentials from environment variables in standard OpenStack form."
      },
      "fetch_until_empty_page": {
        "title": "Fetch Until Empty Page",
        "help": "When paginating, always fetch unless we received an empty page.\n\nConsider using this option if rclone listings show fewer objects\nthan expected, or if repeated syncs copy unchanged objects.\n\nIt is safe to enable this, but rclone may make more API calls than\nnecessary.\n\nThis is one of a pair of workarounds to handle implementations\nof the Swift API that do not implement pagination as expected.  See\nalso \"partial_page_fetch_threshold\"."
      },
      "key": {
        "title": "Key",
        "help": "API key or password (OS_PASSWORD)."
      },
      "leave_parts_on_error": {
        "title": "Leave Parts On Error",
        "help": "If true avoid calling abort upload on a failure.\n\nIt should be set to true for resuming uploads across different sessions."
      },
      "no_chunk": {
        "title": "No Chunk",
        "help": "Don't chunk files during streaming upload.\n\nWhen doing streaming uploads (e.g. using `rcat` or `mount` with\n`--vfs-cache-mode off`) setting this flag will cause the swift backend\nto not upload chunked files.\n\nThis will limit the maximum streamed upload size to 5 GiB. This is\nuseful because non chunked files are easier to deal with and have an\nMD5SUM.\n\nRclone will still chunk files bigger than `chunk_size` when doing\nnormal copy operations."
      },
      "no_large_objects": {
        "title": "No Large Objects",
        "help": "Disable support for static and dynamic large objects\n\nSwift cannot transparently store files bigger than 5 GiB. There are\ntwo schemes for chunking large files, static large objects (SLO) or\ndynamic large objects (DLO), and the API does not allow rclone to\ndetermine whether a file is a static or dynamic large object without\ndoing a HEAD on the object. Since these need to be treated\ndifferently, this means rclone has to issue HEAD requests for objects\nfor example when reading checksums.\n\nWhen `no_large_objects` is set, rclone will assume that there are no\nstatic or dynamic large objects stored. This means it can stop doing\nthe extra HEAD calls which in turn increases performance greatly\nespecially when doing a swift to swift transfer with `--checksum` set.\n\nSetting this option implies `no_chunk` and also that no files will be\nuploaded in chunks, so files bigger than 5 GiB will just fail on\nupload.\n\nIf you set this option and there **are** static or dynamic large objects,\nthen this will give incorrect hashes for them. Downloads will succeed,\nbut other operations such as Remove and Copy will fail.\n"
      },
      "partial_page_fetch_threshold": {
        "title": "Partial Page Fetch Threshold",
        "help": "When paginating, fetch if the current page is within this percentage of the limit.\n\nConsider using this option if rclone listings show fewer objects\nthan expected, or if repeated syncs copy unchanged objects.\n\nIt is safe to enable this, but rclone may make more API calls than\nnecessary.\n\nThis is one of a pair of workarounds to handle implementations\nof the Swift API that do not implement pagination as expected.  See\nalso \"fetch_until_empty_page\"."
      },
      "region": {
        "title": "Region",
        "help": "Region name - optional (OS_REGION_NAME)."
      },
      "storage_policy": {
        "title": "Storage Policy",
        "help": "The storage policy to use when creating a new container.\n\nThis applies the specified storage policy when creating a new\ncontainer. The policy cannot be changed afterwards. The allowed\nconfiguration values and their meaning depend on your Swift storage\nprovider."
      },
      "storage_url": {
        "title": "Storage Url",
        "help": "Storage URL - optional (OS_STORAGE_URL)."
      },
      "tenant": {
        "title": "Tenant",
        "help": "Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)."
      },
      "tenant_domain": {
        "title": "Tenant Domain",
        "help": "Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)."
      },
      "tenant_id": {
        "title": "Tenant Id",
        "help": "Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)."
      },
      "use_segments_container": {
        "title": "Use Segments Container",
        "help": "Choose destination for large object segments\n\nSwift cannot transparently store files bigger than 5 GiB and rclone\nwill chunk files larger than `chunk_size` (default 5 GiB) in order to\nupload them.\n\nIf this value is `true` the chunks will be stored in an additional\ncontainer named the same as the destination container but with\n`_segments` appended. This means that there won't be any duplicated\ndata in the original container but having another container may not be\nacceptable.\n\nIf this value is `false` the chunks will be stored in a\n`.file-segments` directory in the root of the container. This\ndirectory will be omitted when listing the container. Some\nproviders (eg Blomp) require this mode as creating additional\ncontainers isn't allowed. If it is desired to see the `.file-segments`\ndirectory in the root then this flag must be set to `true`.\n\nIf this value is `unset` (the default), then rclone will choose the value\nto use. It will be `false` unless rclone detects any `auth_url`s that\nit knows need it to be `true`. In this case you'll see a message in\nthe DEBUG log.\n"
      },
      "user": {
        "title": "User",
        "help": "User name to log in (OS_USERNAME)."
      },
      "user_id": {
        "title": "User Id",
        "help": "User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID)."
      }
    },
    "tardigrade": {
      "access_grant": {
        "title": "Access Grant",
        "help": "Access grant."
      },
      "api_key": {
        "title": "Api Key",
        "help": "API key."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "passphrase": {
        "title": "Passphrase",
        "help": "Encryption passphrase.\n\nTo access existing objects enter passphrase used for uploading."
      },
      "provider": {
        "title": "Provider",
        "help": "Choose an authentication method."
      },
      "satellite_address": {
        "title": "Satellite Address",
        "help": "Satellite address.\n\nCustom satellite address should match the format: `<nodeid>@<address>:<port>`."
      }
    },
    "ulozto": {
      "app_token": {
        "title": "App Token",
        "help": "The application token identifying the app. An app API key can be either found in the API\ndoc https://uloz.to/upload-resumable-api-beta or obtained from customer service."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "list_page_size": {
        "title": "List Page Size",
        "help": "The size of a single page for list commands. 1-500"
      },
      "password": {
        "title": "Password",
        "help": "The password for the user."
      },
      "root_folder_slug": {
        "title": "Root Folder Slug",
        "help": "If set, rclone will use this folder as the root folder for all operations. For example,\nif the slug identifies 'foo/bar/', 'ulozto:baz' is equivalent to 'ulozto:foo/bar/baz' without\nany root slug set."
      },
      "username": {
        "title": "Username",
        "help": "The username of the principal to operate as."
      }
    },
    "union": {
      "action_policy": {
        "title": "Action Policy",
        "help": "Policy to choose upstream on ACTION category."
      },
      "cache_time": {
        "title": "Cache Time",
        "help": "Cache time of usage and free space (in seconds).\n\nThis option is only useful when a path preserving policy is used."
      },
      "create_policy": {
        "title": "Create Policy",
        "help": "Policy to choose upstream on CREATE category."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "min_free_space": {
        "title": "Min Free Space",
        "help": "Minimum viable free space for lfs/eplfs policies.\n\nIf a remote has less than this much free space then it won't be\nconsidered for use in lfs or eplfs policies."
      },
      "search_policy": {
        "title": "Search Policy",
        "help": "Policy to choose upstream on SEARCH category."
      },
      "upstreams": {
        "title": "Upstreams",
        "help": "List of space separated upstreams.\n\nCan be 'upstreama:test/dir upstreamb:', '\"upstreama:test/space:ro dir\" upstreamb:', etc."
      }
    },
    "uptobox": {
      "access_token": {
        "title": "Access Token",
        "help": "Your access token.\n\nGet it from https://uptobox.com/my_account."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "private": {
        "title": "Private",
        "help": "Set to make uploaded files private"
      }
    },
    "webdav": {
      "auth_redirect": {
        "title": "Auth Redirect",
        "help": "Preserve authentication on redirect.\n\nIf the server redirects rclone to a new domain when it is trying to\nread a file then normally rclone will drop the Authorization: header\nfrom the request.\n\nThis is standard security practice to avoid sending your credentials\nto an unknown webserver.\n\nHowever this is desirable in some circumstances. If you are getting\nan error like \"401 Unauthorized\" when rclone is attempting to read\nfiles from the webdav server then you can try this option.\n"
      },
      "bearer_token": {
        "title": "Bearer Token",
        "help": "Bearer token instead of user/pass (e.g. a Macaroon)."
      },
      "bearer_token_command": {
        "title": "Bearer Token Command",
        "help": "Command to run to get a bearer token."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info.\n\nDefault encoding is Slash,LtGt,DoubleQuote,Colon,Question,Asterisk,Pipe,Hash,Percent,BackSlash,Del,Ctl,LeftSpace,LeftTilde,RightSpace,RightPeriod,InvalidUtf8 for sharepoint-ntlm or identity otherwise."
      },
      "headers": {
        "title": "Headers",
        "help": "Set HTTP headers for all transactions.\n\nUse this to set additional HTTP headers for all transactions\n\nThe input format is comma separated list of key,value pairs.  Standard\n[CSV encoding](https://godoc.org/encoding/csv) may be used.\n\nFor example, to set a Cookie use 'Cookie,name=value', or '\"Cookie\",\"name=value\"'.\n\nYou can set multiple headers, e.g. '\"Cookie\",\"name=value\",\"Authorization\",\"xxx\"'.\n"
      },
      "nextcloud_chunk_size": {
        "title": "Nextcloud Chunk Size",
        "help": "Nextcloud upload chunk size.\n\nWe recommend configuring your NextCloud instance to increase the max chunk size to 1 GB for better upload performances.\nSee https://docs.nextcloud.com/server/latest/admin_manual/configuration_files/big_file_upload_configuration.html#adjust-chunk-size-on-nextcloud-side\n\nSet to 0 to disable chunked uploading.\n"
      },
      "owncloud_exclude_mounts": {
        "title": "Owncloud Exclude Mounts",
        "help": "Exclude ownCloud mounted storages"
      },
      "owncloud_exclude_shares": {
        "title": "Owncloud Exclude Shares",
        "help": "Exclude ownCloud shares"
      },
      "pacer_min_sleep": {
        "title": "Pacer Min Sleep",
        "help": "Minimum time to sleep between API calls."
      },
      "pass": {
        "title": "Pass",
        "help": "Password."
      },
      "unix_socket": {
        "title": "Unix Socket",
        "help": "Path to a unix domain socket to dial to, instead of opening a TCP connection directly"
      },
      "url": {
        "title": "Url",
        "help": "URL of http host to connect to.\n\nE.g. https://example.com."
      },
      "user": {
        "title": "User",
        "help": "User name.\n\nIn case NTLM authentication is used, the username should be in the format 'Domain\\User'."
      },
      "vendor": {
        "title": "Vendor",
        "help": "Name of the WebDAV site/service/software you are using."
      }
    },
    "yandex": {
      "auth_url": {
        "title": "Auth Url",
        "help": "Auth server URL.\n\nLeave blank to use the provider defaults."
      },
      "client_credentials": {
        "title": "Client Credentials",
        "help": "Use client credentials OAuth flow.\n\nThis will use the OAUTH2 client Credentials Flow as described in RFC 6749.\n\nNote that this option is NOT supported by all backends."
      },
      "client_id": {
        "title": "Client Id",
        "help": "OAuth Client Id.\n\nLeave blank normally."
      },
      "client_secret": {
        "title": "Client Secret",
        "help": "OAuth Client Secret.\n\nLeave blank normally."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "hard_delete": {
        "title": "Hard Delete",
        "help": "Delete files permanently rather than putting them into the trash."
      },
      "spoof_ua": {
        "title": "Spoof Ua",
        "help": "Set the user agent to match an official version of the yandex disk client. May help with upload performance."
      },
      "token": {
        "title": "Token",
        "help": "OAuth Access Token as a JSON blob."
      },
      "token_url": {
        "title": "Token Url",
        "help": "Token server url.\n\nLeave blank to use the provider defaults."
      }
    },
    "zoho": {
      "auth_url": {
        "title": "Auth Url",
        "help": "Auth server URL.\n\nLeave blank to use the provider defaults."
      },
      "client_credentials": {
        "title": "Client Credentials",
        "help": "Use client credentials OAuth flow.\n\nThis will use the OAUTH2 client Credentials Flow as described in RFC 6749.\n\nNote that this option is NOT supported by all backends."
      },
      "client_id": {
        "title": "Client Id",
        "help": "OAuth Client Id.\n\nLeave blank normally."
      },
      "client_secret": {
        "title": "Client Secret",
        "help": "OAuth Client Secret.\n\nLeave blank normally."
      },
      "description": {
        "title": "Description",
        "help": "Description of the remote."
      },
      "encoding": {
        "title": "Encoding",
        "help": "The encoding for the backend.\n\nSee the [encoding section in the overview](/overview/#encoding) for more info."
      },
      "region": {
        "title": "Region",
        "help": "Zoho region to connect to.\n\nYou'll have to use the region your organization is registered in. If\nnot sure use the same top level domain as you connect to in your\nbrowser."
      },
      "token": {
        "title": "Token",
        "help": "OAuth Access Token as a JSON blob."
      },
      "token_url": {
        "title": "Token Url",
        "help": "Token server url.\n\nLeave blank to use the provider defaults."
      },
      "upload_cutoff": {
        "title": "Upload Cutoff",
        "help": "Cutoff for switching to large file upload api (>= 10 MiB)."
      }
    }
  }
}
